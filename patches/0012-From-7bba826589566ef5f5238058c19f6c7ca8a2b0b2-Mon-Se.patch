From 9c9c23ff981bf5caf01a35c59ad5ab0d1e79a29f Mon Sep 17 00:00:00 2001
From: robcore <robpatershuk@gmail.com>
Date: Thu, 28 Apr 2016 13:53:58 -0600
Subject: [PATCH 12/50] From 7bba826589566ef5f5238058c19f6c7ca8a2b0b2 Mon Sep
 17 00:00:00 2001 From: Alex Shi <alex.shi@intel.com> Date: Sun, 17 Feb 2013
 14:03:45 +0800 Subject: [PATCH 197/240] sched: add new sg/sd_lb_stats fields
 for incoming  fork/exec/wake balancing

For power aware balancing, we care the sched domain/group's utilization.
So add: sd_lb_stats.sd_util and sg_lb_stats.group_util.

And want to know which group is busiest but still has capability to
handle more tasks, so add: sd_lb_stats.group_leader

Signed-off-by: Alex Shi <alex.shi@intel.com>
---
 kernel/sched/fair.c | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index ef0124c..3c77e22 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3992,6 +3992,10 @@ struct sd_lb_stats {
 	unsigned int  busiest_group_weight;

 	int group_imb; /* Is there imbalance in this sd */
+
+	/* Varibles of power awaring scheduling */
+	unsigned int  sd_util;	/* sum utilization of this domain */
+	struct sched_group *group_leader; /* Group which relieves group_min */
 };

 /*
@@ -4007,6 +4011,7 @@ struct sg_lb_stats {
 	unsigned long group_weight;
 	int group_imb; /* Is there an imbalance in the group ? */
 	int group_has_capacity; /* Is there extra capacity in the group? */
+	unsigned int group_util;	/* sum utilization of group */
 };

 /**
--
2.8.0.rc3
---
 kernel/sched/fair.c      | 5 +++++
 kernel/sched/fair.c.orig | 5 +++++
 2 files changed, 10 insertions(+)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 908e917..6cb4434 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4081,6 +4081,10 @@ struct sd_lb_stats {
 	unsigned int  busiest_group_weight;
 
 	int group_imb; /* Is there imbalance in this sd */
+
+	/* Varibles of power awaring scheduling */
+	unsigned int  sd_util;	/* sum utilization of this domain */
+	struct sched_group *group_leader; /* Group which relieves group_min */
 };
 
 /*
@@ -4096,6 +4100,7 @@ struct sg_lb_stats {
 	unsigned long group_weight;
 	int group_imb; /* Is there an imbalance in the group ? */
 	int group_has_capacity; /* Is there extra capacity in the group? */
+	unsigned int group_util;	/* sum utilization of group */
 };
 
 /**
diff --git a/kernel/sched/fair.c.orig b/kernel/sched/fair.c.orig
index e3e2f36..908e917 100644
--- a/kernel/sched/fair.c.orig
+++ b/kernel/sched/fair.c.orig
@@ -1309,8 +1309,13 @@ static void update_cfs_rq_blocked_load(struct cfs_rq *cfs_rq, int force_update)
 
 static inline void update_rq_runnable_avg(struct rq *rq, int runnable)
 {
+	u32 period;
 	__update_entity_runnable_avg(rq->clock_task, &rq->avg, runnable);
 	__update_tg_runnable_avg(&rq->avg, &rq->cfs);
+
+	period = rq->avg.runnable_avg_period ? rq->avg.runnable_avg_period : 1;
+	rq->util = (u64)(rq->avg.runnable_avg_sum << SCHED_POWER_SHIFT)
+				/ period;
 }
 
 /* Add the load generated by se into cfs_rq's child load-average */
-- 
2.8.0.rc3

