From b3069e04c597df47a62ccbc84dd64cf0b5633972 Mon Sep 17 00:00:00 2001
From: robcore <robpatershuk@gmail.com>
Date: Thu, 28 Apr 2016 13:55:50 -0600
Subject: [PATCH 15/50] From a609807150da3f20b162c28f40cdbf18db4f0b2d Mon Sep
 17 00:00:00 2001 From: Alex Shi <alex.shi@intel.com> Date: Sun, 24 Mar 2013
 21:47:59 +0800 Subject: [PATCH 200/240] sched: get rq potential maximum
 utilization

Since the rt task priority is higher than fair tasks, cfs_rq utilization
is just the left of rt utilization.

When there are some cfs tasks in queue, the potential utilization may
be yielded, so mulitiplying cfs task number to get max potential
utilization of cfs.

Thanks for Paul Turner and Namhyung's reminder!

Signed-off-by: Alex Shi <alex.shi@intel.com>
---
 kernel/sched/fair.c | 17 +++++++++++++++++
 1 file changed, 17 insertions(+)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 5f997bb..ec87666 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3139,6 +3139,23 @@ struct sg_lb_stats {
 	unsigned int group_util;	/* sum utilization of group */
 };

+static unsigned long scale_rt_util(int cpu);
+
+/*
+ * max_cfs_util - get the possible maximum cfs utilization
+ */
+static unsigned int max_cfs_util(int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned int rt_util = scale_rt_util(cpu);
+	unsigned int cfs_util;
+
+	/* yield cfs utilization to rt's, if total utilization > 100% */
+	cfs_util = min(rq->util, (unsigned int)(FULL_UTIL - rt_util));
+
+	return cfs_util * rq->cfs.h_nr_running;
+}
+
 /*
  * sched_balance_self: balance the current task (running on cpu) in domains
  * that have the 'flag' flag set. In practice, this is SD_BALANCE_FORK and
--
2.8.0.rc3
---
 kernel/sched/fair.c      | 17 +++++++++++++++++
 kernel/sched/fair.c.orig | 14 ++++++--------
 2 files changed, 23 insertions(+), 8 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index f99ee74..903e004 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3246,6 +3246,23 @@ struct sg_lb_stats {
 	unsigned int group_util;	/* sum utilization of group */
 };
 
+static unsigned long scale_rt_util(int cpu);
+
+/*
+ * max_cfs_util - get the possible maximum cfs utilization
+ */
+static unsigned int max_cfs_util(int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned int rt_util = scale_rt_util(cpu);
+	unsigned int cfs_util;
+
+	/* yield cfs utilization to rt's, if total utilization > 100% */
+	cfs_util = min(rq->util, (unsigned int)(FULL_UTIL - rt_util));
+
+	return cfs_util * rq->cfs.h_nr_running;
+}
+
 /*
  * sched_balance_self: balance the current task (running on cpu) in domains
  * that have the 'flag' flag set. In practice, this is SD_BALANCE_FORK and
diff --git a/kernel/sched/fair.c.orig b/kernel/sched/fair.c.orig
index 153b3de..f99ee74 100644
--- a/kernel/sched/fair.c.orig
+++ b/kernel/sched/fair.c.orig
@@ -4155,10 +4155,10 @@ unsigned long __weak arch_scale_smt_power(struct sched_domain *sd, int cpu)
 	return default_scale_smt_power(sd, cpu);
 }
 
-unsigned long scale_rt_power(int cpu)
+unsigned long scale_rt_util(int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
-	u64 total, available, age_stamp, avg;
+	u64 total, age_stamp, avg;
 
 	/*
 	 * Since we're reading these variables without serialization make sure
@@ -4170,10 +4170,8 @@ unsigned long scale_rt_power(int cpu)
 	total = sched_avg_period() + (rq->clock - age_stamp);
 
 	if (unlikely(total < avg)) {
-		/* Ensures that power won't end up being negative */
-		available = 0;
-	} else {
-		available = total - avg;
+		/* Ensures rt utilization won't beyond full scaled value */
+		return SCHED_POWER_SCALE;
 	}
 
 	if (unlikely((s64)total < SCHED_POWER_SCALE))
@@ -4181,7 +4179,7 @@ unsigned long scale_rt_power(int cpu)
 
 	total >>= SCHED_POWER_SHIFT;
 
-	return div_u64(available, total);
+	return div_u64(avg, total);
 }
 
 static void update_cpu_power(struct sched_domain *sd, int cpu)
@@ -4208,7 +4206,7 @@ static void update_cpu_power(struct sched_domain *sd, int cpu)
 
 	power >>= SCHED_POWER_SHIFT;
 
-	power *= scale_rt_power(cpu);
+	power *= SCHED_POWER_SCALE - scale_rt_util(cpu);
 	power >>= SCHED_POWER_SHIFT;
 
 	if (!power)
-- 
2.8.0.rc3

