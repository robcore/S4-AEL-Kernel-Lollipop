From 69f7647676665ce13c07c26cda70ac40dc43cd6d Mon Sep 17 00:00:00 2001
From: Vikram Mulukutla <markivx@codeaurora.org>
Date: Mon, 13 Oct 2014 21:29:33 -0700
Subject: [PATCH] idle: Implement a per-cpu idle-polling mode

cpu_idle_poll_ctrl provides a way of switching the
idle thread to use cpu_idle_poll instead of the arch
specific lower power mode callbacks (arch_cpu_idle).
cpu_idle_poll spins on a flag in a tight loop with
interrupts enabled.

In some cases it may be useful to enter the tight loop
polling mode only on a particular CPU. This allows
other CPUs to continue using the arch specific low
power mode callbacks. Provide an API that allows this.

Signed-off-by: Vikram Mulukutla <markivx@codeaurora.org>
Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
---
 include/linux/cpu.h |  1 +
 kernel/cpu/idle.c   | 27 +++++++++++++++++++++++++--
 2 files changed, 26 insertions(+), 2 deletions(-)

diff --git a/include/linux/cpu.h b/include/linux/cpu.h
index eec5a84..2b2e95c 100644
--- a/include/linux/cpu.h
+++ b/include/linux/cpu.h
@@ -288,6 +288,7 @@ void cpu_startup_entry(enum cpuhp_state state);
 void cpu_idle(void);
 
 void cpu_idle_poll_ctrl(bool enable);
+void per_cpu_idle_poll_ctrl(int cpu, bool enable);
 
 void arch_cpu_idle(void);
 void arch_cpu_idle_prepare(void);
diff --git a/kernel/cpu/idle.c b/kernel/cpu/idle.c
index ebfd162..88e6424 100644
--- a/kernel/cpu/idle.c
+++ b/kernel/cpu/idle.c
@@ -6,6 +6,7 @@
 #include <linux/tick.h>
 #include <linux/mm.h>
 #include <linux/stackprotector.h>
+#include <linux/percpu.h>
 
 #include <asm/tlb.h>
 
@@ -26,6 +27,24 @@ void cpu_idle_poll_ctrl(bool enable)
 	mb();
 }
 
+static DEFINE_PER_CPU(int, idle_force_poll);
+
+void per_cpu_idle_poll_ctrl(int cpu, bool enable)
+{
+	if (enable) {
+		per_cpu(idle_force_poll, cpu)++;
+	} else {
+		per_cpu(idle_force_poll, cpu)--;
+		WARN_ON_ONCE(per_cpu(idle_force_poll, cpu) < 0);
+	}
+
+	/*
+	 * Make sure poll mode is entered on the relevant CPU after the flag is
+	 * set
+	 */
+	mb();
+}
+
 #ifdef CONFIG_GENERIC_IDLE_POLL_SETUP
 static int __init cpu_idle_poll_setup(char *__unused)
 {
@@ -47,7 +66,8 @@ static inline int cpu_idle_poll(void)
 	rcu_idle_enter();
 	trace_cpu_idle_rcuidle(0, smp_processor_id());
 	local_irq_enable();
-	while (!tif_need_resched() && cpu_idle_force_poll)
+	while (!tif_need_resched() && (cpu_idle_force_poll ||
+		__get_cpu_var(idle_force_poll)))
 		cpu_relax();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	rcu_idle_exit();
@@ -89,7 +109,9 @@ static void cpu_idle_loop(void)
 			 * know that the IPI is going to arrive right
 			 * away
 			 */
-			if (cpu_idle_force_poll || tick_check_broadcast_expired()) {
+			if (cpu_idle_force_poll ||
+			    tick_check_broadcast_expired() ||
+			    __get_cpu_var(idle_force_poll)) {
 				cpu_idle_poll();
 			} else {
 				if (!current_clr_polling_and_test()) {
@@ -133,5 +155,6 @@ void cpu_startup_entry(enum cpuhp_state state)
 #endif
 	__current_set_polling();
 	arch_cpu_idle_prepare();
+	per_cpu(idle_force_poll, smp_processor_id()) = 0;
 	cpu_idle_loop();
 }
