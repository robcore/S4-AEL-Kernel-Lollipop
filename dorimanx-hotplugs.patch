From 60834efb25485b37924340daef85b66204c563b8 Mon Sep 17 00:00:00 2001
From: Dorimanx <yuri@bynet.co.il>
Date: Fri, 10 Apr 2015 17:24:50 +0300
Subject: [PATCH] Merged all my HOTPLUG + fast charge, from my KK Kernel. see
 history there.

More work to be done to even try to build with all that...
---
 arch/arm/mach-msm/Kconfig           |   52 ++
 arch/arm/mach-msm/Makefile          |    6 +
 arch/arm/mach-msm/alucard_hotplug.c |  968 ++++++++++++++++++++++++
 arch/arm/mach-msm/bricked_hotplug.c |  922 +++++++++++++++++++++++
 arch/arm/mach-msm/cpufreq_limit.c   |  498 +++++++++++++
 arch/arm/mach-msm/fastchg.c         |  211 ++++++
 arch/arm/mach-msm/intelli_hotplug.c |  851 +++++++++++++++++++++
 arch/arm/mach-msm/msm_hotplug.c     | 1403 +++++++++++++++++++++++++++++++++++
 include/linux/fastchg.h             |   41 +
 9 files changed, 4952 insertions(+)
 create mode 100644 arch/arm/mach-msm/alucard_hotplug.c
 create mode 100644 arch/arm/mach-msm/bricked_hotplug.c
 create mode 100644 arch/arm/mach-msm/cpufreq_limit.c
 create mode 100644 arch/arm/mach-msm/fastchg.c
 create mode 100644 arch/arm/mach-msm/intelli_hotplug.c
 create mode 100644 arch/arm/mach-msm/msm_hotplug.c
 create mode 100644 include/linux/fastchg.h

diff --git a/arch/arm/mach-msm/Kconfig b/arch/arm/mach-msm/Kconfig
index b1440cb..b43fa30 100644
--- a/arch/arm/mach-msm/Kconfig
+++ b/arch/arm/mach-msm/Kconfig
@@ -1853,6 +1853,34 @@ config MSM_DALRPC_TEST
 
 if CPU_FREQ_MSM
 
+config MSM_HOTPLUG
+	bool "MSM hotplug driver"
+	depends on HOTPLUG_CPU
+	default y
+	help
+	  The MSM hotplug driver controls on-/offlining of additional cores based
+	  on current cpu load.
+
+config INTELLI_HOTPLUG
+	bool "Enable intelli-plug cpu hotplug driver"
+	default y
+	help
+	  Generic Intelli-plug cpu hotplug driver for ARM SOCs
+
+config ALUCARD_HOTPLUG
+	bool "Enable alucard-hotplug cpu hotplug driver"
+	default y
+	help
+	  Generic Alucard-hotplug cpu hotplug driver for ARM SOCs
+
+config BRICKED_HOTPLUG
+	bool "Enable kernel based mpdecision"
+	depends on MSM_RUN_QUEUE_STATS
+	default y
+	help
+	  This enables kernel based multi core control.
+	  (up/down hotplug based on load)
+
 config MSM_CPU_FREQ_SET_MIN_MAX
 	bool "Set Min/Max CPU frequencies."
 	default n
@@ -1872,6 +1900,23 @@ config MSM_CPU_FREQ_MIN
 
 endif # CPU_FREQ_MSM
 
+config MSM_CPUFREQ_LIMITER
+	tristate "MSM CPU frequency limiter"
+	default y
+	help
+	  This driver limits MSM CPU frequency through sysfs file system.
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_stats.
+
+	  If in doubt, say Y.
+
+config ALUCARD_HOTPLUG_USE_CPU_UTIL
+	bool "Enable alucard-hotplug cpu statistics utilization"
+	depends on (ALUCARD_HOTPLUG)
+	default n
+	help
+	  Alucard-hotplug cpu statistics utilization
+
 config MSM_DEVFREQ_CPUBW
 	bool "Devfreq device for CPU<->DDR IB/AB BW voting"
 	depends on PM_DEVFREQ
@@ -3037,6 +3082,13 @@ config WALL_CLK_SYSFS
 	 Support the wallclk directory in sysfs filesystem to enable the
 	 wall clock simulation and read the current SFN.
 
+config FORCE_FAST_CHARGE
+	bool "Force AC charge mode at will"
+	default y
+	help
+	  A simple sysfs interface to force adapters that
+	  are detected as USB to charge as AC.
+
 config KRAIT_REGULATOR
 	bool "Support Kraits powered via ganged regulators in the pmic"
 	help
diff --git a/arch/arm/mach-msm/Makefile b/arch/arm/mach-msm/Makefile
index d4c4c34..fd6787d 100755
--- a/arch/arm/mach-msm/Makefile
+++ b/arch/arm/mach-msm/Makefile
@@ -424,3 +424,9 @@ obj-$(CONFIG_ARCH_RANDOM) += early_random.o
 obj-$(CONFIG_PERFMAP) += perfmap.o
 
 obj-$(CONFIG_ARCH_MSM8974) += lge_moca_drv.o
+
+obj-$(CONFIG_MSM_CPUFREQ_LIMITER) += cpufreq_limit.o
+obj-$(CONFIG_FORCE_FAST_CHARGE) += fastchg.o
+obj-$(CONFIG_ALUCARD_HOTPLUG) += alucard_hotplug.o
+obj-$(CONFIG_INTELLI_HOTPLUG) += intelli_hotplug.o
+obj-$(CONFIG_BRICKED_HOTPLUG) += bricked_hotplug.o
diff --git a/arch/arm/mach-msm/alucard_hotplug.c b/arch/arm/mach-msm/alucard_hotplug.c
new file mode 100644
index 0000000..f9b3cd3
--- /dev/null
+++ b/arch/arm/mach-msm/alucard_hotplug.c
@@ -0,0 +1,968 @@
+/*
+ * Author: Alucard_24@XDA
+ *
+ * Copyright 2012 Alucard_24@XDA
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+#include <linux/cpu.h>
+#include <linux/cpufreq.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/tick.h>
+#include <linux/sched.h>
+#include <linux/mutex.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include "acpuclock.h"
+
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#else
+#include <linux/fb.h>
+#endif
+
+struct hotplug_cpuinfo {
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+	u64 prev_cpu_wall;
+	u64 prev_cpu_idle;
+#endif
+	unsigned int up_load;
+	unsigned int down_load;
+	unsigned int up_freq;
+	unsigned int down_freq;
+	unsigned int up_rq;
+	unsigned int down_rq;
+	unsigned int up_rate;
+	unsigned int down_rate;
+	unsigned int cur_up_rate;
+	unsigned int cur_down_rate;
+};
+
+static DEFINE_PER_CPU(struct hotplug_cpuinfo, od_hotplug_cpuinfo);
+
+#ifndef CONFIG_POWERSUSPEND
+static struct notifier_block notif;
+#endif
+static struct delayed_work alucard_hotplug_work;
+
+static struct hotplug_tuners {
+	unsigned int hotplug_sampling_rate;
+	unsigned int hotplug_enable;
+	unsigned int min_cpus_online;
+	unsigned int maxcoreslimit;
+	unsigned int maxcoreslimit_sleep;
+	unsigned int hp_io_is_busy;
+	unsigned int hotplug_suspend;
+	bool suspended;
+	bool force_cpu_up;
+	struct mutex alu_hotplug_mutex;
+} hotplug_tuners_ins = {
+	.hotplug_sampling_rate = 30,
+#ifdef CONFIG_MACH_JF
+	.hotplug_enable = 1,
+#else
+	.hotplug_enable = 0,
+#endif
+	.min_cpus_online = 1,
+	.maxcoreslimit = NR_CPUS,
+	.maxcoreslimit_sleep = 1,
+	.hp_io_is_busy = 0,
+	.hotplug_suspend = 0,
+	.suspended = false,
+	.force_cpu_up = false,
+};
+
+#define DOWN_INDEX		(0)
+#define UP_INDEX		(1)
+
+#define RQ_AVG_TIMER_RATE	10
+
+struct runqueue_data {
+	unsigned int nr_run_avg;
+	unsigned int update_rate;
+	int64_t last_time;
+	int64_t total_time;
+	struct delayed_work work;
+	struct workqueue_struct *nr_run_wq;
+	spinlock_t lock;
+};
+
+static struct runqueue_data *rq_data;
+static void rq_work_fn(struct work_struct *work);
+
+static void start_rq_work(void)
+{
+	rq_data->nr_run_avg = 0;
+	rq_data->last_time = 0;
+	rq_data->total_time = 0;
+	if (rq_data->nr_run_wq == NULL)
+		rq_data->nr_run_wq =
+			create_singlethread_workqueue("nr_run_avg");
+
+	mod_delayed_work_on(BOOT_CPU, rq_data->nr_run_wq, &rq_data->work,
+			   msecs_to_jiffies(rq_data->update_rate));
+	return;
+}
+
+static void stop_rq_work(void)
+{
+	if (rq_data->nr_run_wq)
+		cancel_delayed_work(&rq_data->work);
+	return;
+}
+
+static int init_rq_avg(void)
+{
+	rq_data = kzalloc(sizeof(struct runqueue_data), GFP_KERNEL);
+	if (rq_data == NULL) {
+		pr_err("%s cannot allocate memory\n", __func__);
+		return -ENOMEM;
+	}
+	spin_lock_init(&rq_data->lock);
+	rq_data->update_rate = RQ_AVG_TIMER_RATE;
+	INIT_DEFERRABLE_WORK(&rq_data->work, rq_work_fn);
+
+	return 0;
+}
+
+static void exit_rq_avg(void)
+{
+	kfree(rq_data);
+}
+
+static void rq_work_fn(struct work_struct *work)
+{
+	int64_t time_diff = 0;
+	int64_t nr_run = 0;
+	unsigned long flags = 0;
+	int64_t cur_time = ktime_to_ns(ktime_get());
+
+	spin_lock_irqsave(&rq_data->lock, flags);
+
+	if (rq_data->last_time == 0)
+		rq_data->last_time = cur_time;
+	if (rq_data->nr_run_avg == 0)
+		rq_data->total_time = 0;
+
+	nr_run = nr_running() * 100;
+	time_diff = cur_time - rq_data->last_time;
+	do_div(time_diff, 1000 * 1000);
+
+	if (time_diff != 0 && rq_data->total_time != 0) {
+		nr_run = (nr_run * time_diff) +
+			(rq_data->nr_run_avg * rq_data->total_time);
+		do_div(nr_run, rq_data->total_time + time_diff);
+	}
+	rq_data->nr_run_avg = nr_run;
+	rq_data->total_time += time_diff;
+	rq_data->last_time = cur_time;
+
+	if (rq_data->update_rate != 0)
+		mod_delayed_work_on(BOOT_CPU, rq_data->nr_run_wq, &rq_data->work,
+				   msecs_to_jiffies(rq_data->update_rate));
+
+	spin_unlock_irqrestore(&rq_data->lock, flags);
+}
+
+static unsigned int get_nr_run_avg(void)
+{
+	unsigned int nr_run_avg;
+	unsigned long flags = 0;
+
+	spin_lock_irqsave(&rq_data->lock, flags);
+	nr_run_avg = rq_data->nr_run_avg;
+	rq_data->nr_run_avg = 0;
+	spin_unlock_irqrestore(&rq_data->lock, flags);
+
+	return nr_run_avg;
+}
+
+static void __ref hotplug_work_fn(struct work_struct *work)
+{
+	unsigned int upmaxcoreslimit = 0;
+	unsigned int min_cpus_online = hotplug_tuners_ins.min_cpus_online;
+	unsigned int cpu = 0;
+	int online_cpu = 0;
+	int offline_cpu = 0;
+	int online_cpus = 0;
+	unsigned int rq_avg;
+	bool force_up = hotplug_tuners_ins.force_cpu_up;
+	int io_busy = hotplug_tuners_ins.hp_io_is_busy;
+	cpumask_var_t cpus;
+
+	rq_avg = get_nr_run_avg();
+
+	if (hotplug_tuners_ins.suspended)
+		upmaxcoreslimit = hotplug_tuners_ins.maxcoreslimit_sleep;
+	else
+		upmaxcoreslimit = hotplug_tuners_ins.maxcoreslimit;
+
+	cpumask_copy(cpus, cpu_online_mask);
+	online_cpus = cpumask_weight(cpus);
+
+	for_each_cpu(cpu, cpus) {
+		struct hotplug_cpuinfo *pcpu_info =
+				&per_cpu(od_hotplug_cpuinfo, cpu);
+		unsigned int upcpu = (cpu + 1);
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+		u64 cur_wall_time, cur_idle_time;
+		unsigned int wall_time, idle_time;
+#endif
+		unsigned int cur_load = 0;
+		unsigned int cur_freq = 0;
+		int ret;
+
+#ifdef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+		cur_load = cpufreq_quick_get_util(cpu);
+#else
+		cur_idle_time = get_cpu_idle_time(
+				cpu, &cur_wall_time, io_busy);
+
+		wall_time = (unsigned int)
+				(cur_wall_time -
+					pcpu_info->prev_cpu_wall);
+		pcpu_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+				(cur_idle_time -
+					pcpu_info->prev_cpu_idle);
+		pcpu_info->prev_cpu_idle = cur_idle_time;
+
+		/* if wall_time < idle_time or wall_time == 0, evaluate cpu load next time */
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		cur_load = 100 * (wall_time - idle_time) / wall_time;
+#endif
+
+		/* get the cpu current frequency */
+		/* cur_freq = acpuclk_get_rate(cpu); */
+		cur_freq = cpufreq_quick_get(cpu);
+
+		if (cpu > 0
+			&& ((online_cpus - offline_cpu) > upmaxcoreslimit)) {
+				ret = cpu_down(cpu);
+				if (!ret) {
+#ifdef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+					pcpu_info->cur_up_rate = 1;
+					pcpu_info->cur_down_rate = 1;
+#endif
+					++offline_cpu;
+				}
+		} else if (force_up == true ||
+				(online_cpus + online_cpu) < min_cpus_online) {
+				if (upcpu < upmaxcoreslimit) {
+					if (cpu_is_offline(upcpu)) {
+						ret = cpu_up(upcpu);
+						if (!ret) {
+							pcpu_info->cur_up_rate = 1;
+							pcpu_info->cur_down_rate = 1;
+							++online_cpu;
+						}
+					}
+				}
+		} else if (upcpu > 0
+			&& upcpu < upmaxcoreslimit
+			&& (cpu_is_offline(upcpu))
+			&& (online_cpus + online_cpu) < upmaxcoreslimit
+		    && cur_load >= pcpu_info->up_load
+			&& cur_freq >= pcpu_info->up_freq
+			&& rq_avg > pcpu_info->up_rq) {
+				if (pcpu_info->cur_up_rate %
+						pcpu_info->up_rate == 0) {
+#if 0
+					pr_info("CPU[%u], UPCPU[%u], \
+						cur_freq[%u], cur_load[%u], \
+						rq_avg[%u], up_rate[%u]\n",
+						cpu, upcpu, cur_freq,
+						cur_load, rq_avg,
+						pcpu_info->cur_up_rate);
+#endif
+					ret = cpu_up(upcpu);
+					if (!ret) {
+						pcpu_info->cur_up_rate = 1;
+						pcpu_info->cur_down_rate = 1;
+						++online_cpu;
+					}
+				} else {
+					if (pcpu_info->cur_up_rate < pcpu_info->up_rate)
+						++pcpu_info->cur_up_rate;
+					else
+						pcpu_info->cur_up_rate = 1;
+				}
+		} else if (cpu >= min_cpus_online && (
+				cur_load < pcpu_info->down_load
+				|| (cur_freq <= pcpu_info->down_freq
+				&& rq_avg <= pcpu_info->down_rq))) {
+					if (pcpu_info->cur_down_rate %
+							pcpu_info->down_rate == 0) {
+#if 0
+						pr_info("CPU[%u], \
+							cur_freq[%u], \
+							cur_load[%u], \
+							rq_avg[%u], \
+							down_rate[%u]\n",
+							cpu, cur_freq,
+							cur_load, rq_avg,
+							pcpu_info->
+							cur_down_rate);
+#endif
+						ret = cpu_down(cpu);
+						if (!ret) {
+#ifdef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+							pcpu_info->cur_up_rate = 1;
+							pcpu_info->cur_down_rate = 1;
+#endif
+							++offline_cpu;
+						}
+					} else {
+						if (pcpu_info->cur_down_rate < pcpu_info->down_rate)
+							++pcpu_info->cur_down_rate;
+						else
+							pcpu_info->cur_down_rate = 1;
+					}
+		} else {
+			pcpu_info->cur_up_rate = 1;
+			pcpu_info->cur_down_rate = 1;
+		}
+	}
+
+	if (force_up == true)
+		hotplug_tuners_ins.force_cpu_up = false;
+
+	mod_delayed_work_on(BOOT_CPU, system_wq,
+				&alucard_hotplug_work,
+				msecs_to_jiffies(
+				hotplug_tuners_ins.hotplug_sampling_rate));
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __alucard_hotplug_suspend(struct power_suspend *handler)
+#else
+static void __alucard_hotplug_suspend(void)
+#endif
+{
+	if (hotplug_tuners_ins.hotplug_enable > 0
+				&& hotplug_tuners_ins.hotplug_suspend == 1 &&
+				hotplug_tuners_ins.suspended == false) {
+			mutex_lock(&hotplug_tuners_ins.alu_hotplug_mutex);
+			hotplug_tuners_ins.suspended = true;
+			mutex_unlock(&hotplug_tuners_ins.alu_hotplug_mutex);
+			pr_info("Alucard HotPlug suspended.\n");
+	}
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __ref __alucard_hotplug_resume(struct power_suspend *handler)
+#else
+static void __ref __alucard_hotplug_resume(void)
+#endif
+{
+	if (hotplug_tuners_ins.hotplug_enable > 0
+		&& hotplug_tuners_ins.hotplug_suspend == 1) {
+			mutex_lock(&hotplug_tuners_ins.alu_hotplug_mutex);
+			hotplug_tuners_ins.suspended = false;
+			/* wake up everyone */
+			hotplug_tuners_ins.force_cpu_up = true;
+			mutex_unlock(&hotplug_tuners_ins.alu_hotplug_mutex);
+			pr_info("Alucard HotPlug Resumed.\n");
+	}
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static struct power_suspend alucard_hotplug_power_suspend_driver = {
+	.suspend = __alucard_hotplug_suspend,
+	.resume = __alucard_hotplug_resume,
+};
+#else
+static int prev_fb = FB_BLANK_UNBLANK;
+
+static int fb_notifier_callback(struct notifier_block *self,
+				unsigned long event, void *data)
+{
+	struct fb_event *evdata = data;
+	int *blank;
+
+	if (evdata && evdata->data && event == FB_EVENT_BLANK) {
+		blank = evdata->data;
+		switch (*blank) {
+			case FB_BLANK_UNBLANK:
+				if (prev_fb == FB_BLANK_POWERDOWN) {
+					/* display on */
+					__alucard_hotplug_resume();
+					prev_fb = FB_BLANK_UNBLANK;
+				}
+				break;
+			case FB_BLANK_POWERDOWN:
+				if (prev_fb == FB_BLANK_UNBLANK) {
+					/* display off */
+					__alucard_hotplug_suspend();
+					prev_fb = FB_BLANK_POWERDOWN;
+				}
+				break;
+		}
+	}
+
+	return NOTIFY_OK;
+}
+#endif
+
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+static int alucard_hotplug_callback(struct notifier_block *nb,
+			unsigned long action, void *data)
+{
+	struct hotplug_cpuinfo *pcpu_info;
+	unsigned int cpu = (int)data;
+
+	switch (action & (~CPU_TASKS_FROZEN)) {
+	case CPU_ONLINE:
+		pcpu_info = &per_cpu(od_hotplug_cpuinfo, cpu);
+		pcpu_info->prev_cpu_wall = ktime_to_us(ktime_get());
+		pcpu_info->prev_cpu_idle = get_cpu_idle_time(cpu,
+				&pcpu_info->prev_cpu_wall,
+				hotplug_tuners_ins.hp_io_is_busy);
+		pcpu_info->cur_up_rate = 1;
+		pcpu_info->cur_down_rate = 1;
+		break;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block alucard_hotplug_nb =
+{
+   .notifier_call = alucard_hotplug_callback,
+};
+#endif
+
+static int hotplug_start(void)
+{
+	unsigned int cpu;
+	int ret = 0;
+
+	ret = init_rq_avg();
+	if (ret) {
+		return ret;
+	}
+
+	hotplug_tuners_ins.suspended = false;
+	hotplug_tuners_ins.force_cpu_up = false;
+
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+	get_online_cpus();
+	register_hotcpu_notifier(&alucard_hotplug_nb);
+#endif
+	for_each_possible_cpu(cpu) {
+		struct hotplug_cpuinfo *pcpu_info =
+				&per_cpu(od_hotplug_cpuinfo, cpu);
+
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+		if (cpu_online(cpu)) {
+			pcpu_info->prev_cpu_wall = ktime_to_us(ktime_get());
+			pcpu_info->prev_cpu_idle = get_cpu_idle_time(cpu,
+					&pcpu_info->prev_cpu_wall,
+					hotplug_tuners_ins.hp_io_is_busy);
+		}
+#endif
+		pcpu_info->cur_up_rate = 1;
+		pcpu_info->cur_down_rate = 1;
+	}
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+	put_online_cpus();
+#endif
+
+	start_rq_work();
+
+	INIT_DEFERRABLE_WORK(&alucard_hotplug_work, hotplug_work_fn);
+	mod_delayed_work_on(BOOT_CPU, system_wq,
+				&alucard_hotplug_work,
+				msecs_to_jiffies(
+				hotplug_tuners_ins.hotplug_sampling_rate));
+
+	mutex_init(&hotplug_tuners_ins.alu_hotplug_mutex);
+#ifdef CONFIG_POWERSUSPEND
+	register_power_suspend(&alucard_hotplug_power_suspend_driver);
+#else
+	notif.notifier_call = fb_notifier_callback;
+	if (fb_register_client(&notif))
+		pr_err("Failed to register FB notifier callback for Alucard Hotplug\n");
+#endif
+
+	return 0;
+}
+
+static void hotplug_stop(void)
+{
+	mutex_destroy(&hotplug_tuners_ins.alu_hotplug_mutex);
+	cancel_delayed_work_sync(&alucard_hotplug_work);
+#ifdef CONFIG_POWERSUSPEND
+	unregister_power_suspend(&alucard_hotplug_power_suspend_driver);
+#else
+	fb_unregister_client(&notif);
+	notif.notifier_call = NULL;
+#endif
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+	get_online_cpus();
+	unregister_hotcpu_notifier(&alucard_hotplug_nb);
+	put_online_cpus();
+#endif
+	stop_rq_work();
+	exit_rq_avg();
+}
+
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n",					\
+			hotplug_tuners_ins.object);			\
+}
+
+show_one(hotplug_sampling_rate, hotplug_sampling_rate);
+show_one(hotplug_enable, hotplug_enable);
+show_one(min_cpus_online, min_cpus_online);
+show_one(maxcoreslimit, maxcoreslimit);
+show_one(maxcoreslimit_sleep, maxcoreslimit_sleep);
+show_one(hp_io_is_busy, hp_io_is_busy);
+show_one(hotplug_suspend, hotplug_suspend);
+
+#define show_pcpu_param(file_name, var_name, num_core)			\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	struct hotplug_cpuinfo *pcpu_info =				\
+			&per_cpu(od_hotplug_cpuinfo, num_core - 1);	\
+	return sprintf(buf, "%u\n",					\
+			pcpu_info->var_name);				\
+}
+
+show_pcpu_param(hotplug_freq_1_1, up_freq, 1);
+show_pcpu_param(hotplug_freq_2_1, up_freq, 2);
+show_pcpu_param(hotplug_freq_3_1, up_freq, 3);
+show_pcpu_param(hotplug_freq_2_0, down_freq, 2);
+show_pcpu_param(hotplug_freq_3_0, down_freq, 3);
+show_pcpu_param(hotplug_freq_4_0, down_freq, 4);
+
+show_pcpu_param(hotplug_load_1_1, up_load, 1);
+show_pcpu_param(hotplug_load_2_1, up_load, 2);
+show_pcpu_param(hotplug_load_3_1, up_load, 3);
+show_pcpu_param(hotplug_load_2_0, down_load, 2);
+show_pcpu_param(hotplug_load_3_0, down_load, 3);
+show_pcpu_param(hotplug_load_4_0, down_load, 4);
+
+show_pcpu_param(hotplug_rq_1_1, up_rq, 1);
+show_pcpu_param(hotplug_rq_2_1, up_rq, 2);
+show_pcpu_param(hotplug_rq_3_1, up_rq, 3);
+show_pcpu_param(hotplug_rq_2_0, down_rq, 2);
+show_pcpu_param(hotplug_rq_3_0, down_rq, 3);
+show_pcpu_param(hotplug_rq_4_0, down_rq, 4);
+
+show_pcpu_param(hotplug_rate_1_1, up_rate, 1);
+show_pcpu_param(hotplug_rate_2_1, up_rate, 2);
+show_pcpu_param(hotplug_rate_3_1, up_rate, 3);
+show_pcpu_param(hotplug_rate_2_0, down_rate, 2);
+show_pcpu_param(hotplug_rate_3_0, down_rate, 3);
+show_pcpu_param(hotplug_rate_4_0, down_rate, 4);
+
+#define store_pcpu_param(file_name, var_name, num_core)			\
+static ssize_t store_##file_name					\
+(struct kobject *kobj, struct attribute *attr,				\
+	const char *buf, size_t count)					\
+{									\
+	unsigned int input;						\
+	struct hotplug_cpuinfo *pcpu_info;				\
+	int ret;							\
+									\
+	ret = sscanf(buf, "%u", &input);				\
+	if (ret != 1)							\
+		return -EINVAL;						\
+									\
+	pcpu_info = &per_cpu(od_hotplug_cpuinfo, num_core - 1);		\
+									\
+	if (input == pcpu_info->var_name) {				\
+		return count;						\
+	}								\
+									\
+	pcpu_info->var_name = input;					\
+	return count;							\
+}
+
+store_pcpu_param(hotplug_freq_1_1, up_freq, 1);
+store_pcpu_param(hotplug_freq_2_1, up_freq, 2);
+store_pcpu_param(hotplug_freq_3_1, up_freq, 3);
+store_pcpu_param(hotplug_freq_2_0, down_freq, 2);
+store_pcpu_param(hotplug_freq_3_0, down_freq, 3);
+store_pcpu_param(hotplug_freq_4_0, down_freq, 4);
+
+store_pcpu_param(hotplug_load_1_1, up_load, 1);
+store_pcpu_param(hotplug_load_2_1, up_load, 2);
+store_pcpu_param(hotplug_load_3_1, up_load, 3);
+store_pcpu_param(hotplug_load_2_0, down_load, 2);
+store_pcpu_param(hotplug_load_3_0, down_load, 3);
+store_pcpu_param(hotplug_load_4_0, down_load, 4);
+
+store_pcpu_param(hotplug_rq_1_1, up_rq, 1);
+store_pcpu_param(hotplug_rq_2_1, up_rq, 2);
+store_pcpu_param(hotplug_rq_3_1, up_rq, 3);
+store_pcpu_param(hotplug_rq_2_0, down_rq, 2);
+store_pcpu_param(hotplug_rq_3_0, down_rq, 3);
+store_pcpu_param(hotplug_rq_4_0, down_rq, 4);
+
+store_pcpu_param(hotplug_rate_1_1, up_rate, 1);
+store_pcpu_param(hotplug_rate_2_1, up_rate, 2);
+store_pcpu_param(hotplug_rate_3_1, up_rate, 3);
+store_pcpu_param(hotplug_rate_2_0, down_rate, 2);
+store_pcpu_param(hotplug_rate_3_0, down_rate, 3);
+store_pcpu_param(hotplug_rate_4_0, down_rate, 4);
+
+define_one_global_rw(hotplug_freq_1_1);
+define_one_global_rw(hotplug_freq_2_0);
+define_one_global_rw(hotplug_freq_2_1);
+define_one_global_rw(hotplug_freq_3_0);
+define_one_global_rw(hotplug_freq_3_1);
+define_one_global_rw(hotplug_freq_4_0);
+
+define_one_global_rw(hotplug_load_1_1);
+define_one_global_rw(hotplug_load_2_0);
+define_one_global_rw(hotplug_load_2_1);
+define_one_global_rw(hotplug_load_3_0);
+define_one_global_rw(hotplug_load_3_1);
+define_one_global_rw(hotplug_load_4_0);
+
+define_one_global_rw(hotplug_rq_1_1);
+define_one_global_rw(hotplug_rq_2_0);
+define_one_global_rw(hotplug_rq_2_1);
+define_one_global_rw(hotplug_rq_3_0);
+define_one_global_rw(hotplug_rq_3_1);
+define_one_global_rw(hotplug_rq_4_0);
+
+define_one_global_rw(hotplug_rate_1_1);
+define_one_global_rw(hotplug_rate_2_0);
+define_one_global_rw(hotplug_rate_2_1);
+define_one_global_rw(hotplug_rate_3_0);
+define_one_global_rw(hotplug_rate_3_1);
+define_one_global_rw(hotplug_rate_4_0);
+
+static void cpus_hotplugging(int status) {
+	int ret = 0;
+
+	if (status) {
+		ret = hotplug_start();
+		if (ret)
+			status = 0;
+	} else {
+		hotplug_stop();
+	}
+
+	hotplug_tuners_ins.hotplug_enable = status;
+}
+
+/* hotplug_sampling_rate */
+static ssize_t store_hotplug_sampling_rate(struct kobject *a,
+				struct attribute *b,
+				const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input, 10);
+
+	if (input == hotplug_tuners_ins.hotplug_sampling_rate)
+		return count;
+
+	hotplug_tuners_ins.hotplug_sampling_rate = input;
+
+	return count;
+}
+
+/* hotplug_enable */
+static ssize_t store_hotplug_enable(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = input > 0;
+
+	if (hotplug_tuners_ins.hotplug_enable == input)
+		return count;
+
+	if (input > 0)
+		cpus_hotplugging(1);
+	else
+		cpus_hotplugging(0);
+
+	return count;
+}
+
+/* min_cpus_online */
+static ssize_t store_min_cpus_online(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input > NR_CPUS ? NR_CPUS : input, 1);
+
+	if (hotplug_tuners_ins.min_cpus_online == input)
+		return count;
+
+	hotplug_tuners_ins.min_cpus_online = input;
+
+	return count;
+}
+
+/* maxcoreslimit */
+static ssize_t store_maxcoreslimit(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input > NR_CPUS ? NR_CPUS : input, 1);
+
+	if (hotplug_tuners_ins.maxcoreslimit == input)
+		return count;
+
+	hotplug_tuners_ins.maxcoreslimit = input;
+
+	return count;
+}
+
+/* maxcoreslimit_sleep */
+static ssize_t store_maxcoreslimit_sleep(struct kobject *a,
+				struct attribute *b,
+				const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input > NR_CPUS ? NR_CPUS : input, 1);
+
+	if (hotplug_tuners_ins.maxcoreslimit_sleep == input)
+		return count;
+
+	hotplug_tuners_ins.maxcoreslimit_sleep = input;
+
+	return count;
+}
+
+/* hp_io_is_busy */
+static ssize_t store_hp_io_is_busy(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == hotplug_tuners_ins.hp_io_is_busy)
+		return count;
+
+	hotplug_tuners_ins.hp_io_is_busy = !!input;
+#ifndef CONFIG_ALUCARD_HOTPLUG_USE_CPU_UTIL
+	/* we need to re-evaluate prev_cpu_idle */
+	if (hotplug_tuners_ins.hotplug_enable > 0) {
+		for_each_online_cpu(j) {
+			struct hotplug_cpuinfo *pcpu_info =
+					&per_cpu(od_hotplug_cpuinfo, j);
+			pcpu_info->prev_cpu_idle = get_cpu_idle_time(j,
+					&pcpu_info->prev_cpu_wall,
+					hotplug_tuners_ins.hp_io_is_busy);
+		}
+	}
+#endif
+	return count;
+}
+
+/*
+ * hotplug_suspend control
+ * if set = 1 hotplug will sleep,
+ * if set = 0, then hoplug will be active all the time.
+ */
+static ssize_t store_hotplug_suspend(struct kobject *a,
+				struct attribute *b,
+				const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = input > 0;
+
+	if (hotplug_tuners_ins.hotplug_suspend == input)
+		return count;
+
+	if (input > 0)
+		hotplug_tuners_ins.hotplug_suspend = 1;
+	else {
+		hotplug_tuners_ins.hotplug_suspend = 0;
+		hotplug_tuners_ins.suspended = false;
+	}
+
+	return count;
+}
+
+define_one_global_rw(hotplug_sampling_rate);
+define_one_global_rw(hotplug_enable);
+define_one_global_rw(min_cpus_online);
+define_one_global_rw(maxcoreslimit);
+define_one_global_rw(maxcoreslimit_sleep);
+define_one_global_rw(hp_io_is_busy);
+define_one_global_rw(hotplug_suspend);
+
+static struct attribute *alucard_hotplug_attributes[] = {
+	&hotplug_sampling_rate.attr,
+	&hotplug_enable.attr,
+	&hotplug_freq_1_1.attr,
+	&hotplug_freq_2_0.attr,
+	&hotplug_freq_2_1.attr,
+	&hotplug_freq_3_0.attr,
+	&hotplug_freq_3_1.attr,
+	&hotplug_freq_4_0.attr,
+	&hotplug_load_1_1.attr,
+	&hotplug_load_2_0.attr,
+	&hotplug_load_2_1.attr,
+	&hotplug_load_3_0.attr,
+	&hotplug_load_3_1.attr,
+	&hotplug_load_4_0.attr,
+	&hotplug_rq_1_1.attr,
+	&hotplug_rq_2_0.attr,
+	&hotplug_rq_2_1.attr,
+	&hotplug_rq_3_0.attr,
+	&hotplug_rq_3_1.attr,
+	&hotplug_rq_4_0.attr,
+	&hotplug_rate_1_1.attr,
+	&hotplug_rate_2_0.attr,
+	&hotplug_rate_2_1.attr,
+	&hotplug_rate_3_0.attr,
+	&hotplug_rate_3_1.attr,
+	&hotplug_rate_4_0.attr,
+	&min_cpus_online.attr,
+	&maxcoreslimit.attr,
+	&maxcoreslimit_sleep.attr,
+	&hp_io_is_busy.attr,
+	&hotplug_suspend.attr,
+	NULL
+};
+
+static struct attribute_group alucard_hotplug_attr_group = {
+	.attrs = alucard_hotplug_attributes,
+	.name = "alucard_hotplug",
+};
+
+static int __init alucard_hotplug_init(void)
+{
+	int ret;
+	unsigned int cpu;
+	unsigned int hotplug_freq[NR_CPUS][2] = {
+#ifdef CONFIG_MACH_LGE
+		{0, 1497600},
+		{652800, 1190400},
+		{652800, 1190400},
+		{652800, 0}
+#else
+		{0, 1242000},
+		{810000, 1566000},
+		{918000, 1674000},
+		{1026000, 0}
+#endif
+	};
+	unsigned int hotplug_load[NR_CPUS][2] = {
+		{0, 60},
+		{30, 65},
+		{30, 65},
+		{30, 0}
+	};
+	unsigned int hotplug_rq[NR_CPUS][2] = {
+		{0, 100},
+		{100, 200},
+		{200, 300},
+		{300, 0}
+	};
+	unsigned int hotplug_rate[NR_CPUS][2] = {
+		{1, 1},
+		{4, 1},
+		{4, 1},
+		{4, 1}
+	};
+
+	ret = sysfs_create_group(kernel_kobj, &alucard_hotplug_attr_group);
+	if (ret) {
+		printk(KERN_ERR "failed at(%d)\n", __LINE__);
+		return ret;
+	}
+
+	/* INITIALIZE PCPU VARS */
+	for_each_possible_cpu(cpu) {
+		struct hotplug_cpuinfo *pcpu_info =
+				&per_cpu(od_hotplug_cpuinfo, cpu);
+
+		pcpu_info->up_freq = hotplug_freq[cpu][UP_INDEX];
+		pcpu_info->down_freq = hotplug_freq[cpu][DOWN_INDEX];
+		pcpu_info->up_load = hotplug_load[cpu][UP_INDEX];
+		pcpu_info->down_load = hotplug_load[cpu][DOWN_INDEX];
+		pcpu_info->up_rq = hotplug_rq[cpu][UP_INDEX];
+		pcpu_info->down_rq = hotplug_rq[cpu][DOWN_INDEX];
+		pcpu_info->up_rate = hotplug_rate[cpu][UP_INDEX];
+		pcpu_info->down_rate = hotplug_rate[cpu][DOWN_INDEX];
+	}
+
+	if (hotplug_tuners_ins.hotplug_enable > 0) {
+		hotplug_start();
+	}
+
+	return ret;
+}
+
+static void __exit alucard_hotplug_exit(void)
+{
+	if (hotplug_tuners_ins.hotplug_enable > 0) {
+		hotplug_stop();
+	}
+
+	sysfs_remove_group(kernel_kobj, &alucard_hotplug_attr_group);
+}
+MODULE_AUTHOR("Alucard_24@XDA");
+MODULE_DESCRIPTION("'alucard_hotplug' - A cpu hotplug driver for "
+	"capable processors");
+MODULE_LICENSE("GPL");
+
+late_initcall(alucard_hotplug_init);
diff --git a/arch/arm/mach-msm/bricked_hotplug.c b/arch/arm/mach-msm/bricked_hotplug.c
new file mode 100644
index 0000000..6c2ecc2
--- /dev/null
+++ b/arch/arm/mach-msm/bricked_hotplug.c
@@ -0,0 +1,922 @@
+/*
+ * Bricked Hotplug Driver
+ *
+ * Copyright (c) 2013-2014, Dennis Rassmann <showp1984@gmail.com>
+ * Copyright (c) 2013-2014, Pranav Vashi <neobuddy89@gmail.com>
+ * Copyright (c) 2010-2014, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/workqueue.h>
+#include <linux/completion.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <asm-generic/cputime.h>
+#include <linux/hrtimer.h>
+#include <linux/delay.h>
+#include <linux/platform_device.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#else
+#include <linux/fb.h>
+#endif
+
+#define DEBUG 0
+
+#define MPDEC_TAG			"bricked_hotplug"
+#define HOTPLUG_ENABLED			0
+#define MSM_MPDEC_STARTDELAY		10000
+#define MSM_MPDEC_DELAY			130
+#define DEFAULT_MIN_CPUS_ONLINE		1
+#define DEFAULT_MAX_CPUS_ONLINE		NR_CPUS
+#define DEFAULT_MAX_CPUS_ONLINE_SUSP	1
+#define DEFAULT_SUSPEND_DEFER_TIME	10
+#define DEFAULT_DOWN_LOCK_DUR		500
+
+#define MSM_MPDEC_IDLE_FREQ		499200
+
+enum {
+	MSM_MPDEC_DISABLED = 0,
+	MSM_MPDEC_IDLE,
+	MSM_MPDEC_DOWN,
+	MSM_MPDEC_UP,
+};
+
+#ifndef CONFIG_POWERSUSPEND
+static struct notifier_block notif;
+#endif
+static struct delayed_work hotplug_work;
+static struct delayed_work suspend_work;
+static struct work_struct resume_work;
+static struct workqueue_struct *hotplug_wq;
+static struct workqueue_struct *susp_wq;
+
+static struct cpu_hotplug {
+	unsigned int startdelay;
+	unsigned int suspended;
+	unsigned int suspend_defer_time;
+	unsigned int min_cpus_online_res;
+	unsigned int max_cpus_online_res;
+	unsigned int max_cpus_online_susp;
+	unsigned int delay;
+	unsigned int down_lock_dur;
+	unsigned long int idle_freq;
+	unsigned int max_cpus_online;
+	unsigned int min_cpus_online;
+	unsigned int bricked_enabled;
+	unsigned int hotplug_suspend;
+	struct mutex bricked_hotplug_mutex;
+	struct mutex bricked_cpu_mutex;
+} hotplug = {
+	.startdelay = MSM_MPDEC_STARTDELAY,
+	.suspended = 0,
+	.suspend_defer_time = DEFAULT_SUSPEND_DEFER_TIME,
+	.min_cpus_online_res = DEFAULT_MIN_CPUS_ONLINE,
+	.max_cpus_online_res = DEFAULT_MAX_CPUS_ONLINE,
+	.max_cpus_online_susp = DEFAULT_MAX_CPUS_ONLINE_SUSP,
+	.delay = MSM_MPDEC_DELAY,
+	.down_lock_dur = DEFAULT_DOWN_LOCK_DUR,
+	.idle_freq = MSM_MPDEC_IDLE_FREQ,
+	.max_cpus_online = DEFAULT_MAX_CPUS_ONLINE,
+	.min_cpus_online = DEFAULT_MIN_CPUS_ONLINE,
+	.bricked_enabled = HOTPLUG_ENABLED,
+	.hotplug_suspend = 1,
+};
+
+static unsigned int NwNs_Threshold[8] = {12, 0, 25, 7, 30, 10, 0, 18};
+static unsigned int TwTs_Threshold[8] = {140, 0, 140, 190, 140, 190, 0, 190};
+
+struct down_lock {
+	unsigned int locked;
+	struct delayed_work lock_rem;
+};
+static DEFINE_PER_CPU(struct down_lock, lock_info);
+
+static void apply_down_lock(unsigned int cpu)
+{
+	struct down_lock *dl = &per_cpu(lock_info, cpu);
+
+	dl->locked = 1;
+	queue_delayed_work_on(0, hotplug_wq, &dl->lock_rem,
+			      msecs_to_jiffies(hotplug.down_lock_dur));
+}
+
+static void remove_down_lock(struct work_struct *work)
+{
+	struct down_lock *dl = container_of(work, struct down_lock,
+					    lock_rem.work);
+	dl->locked = 0;
+}
+
+static int check_down_lock(unsigned int cpu)
+{
+	struct down_lock *dl = &per_cpu(lock_info, cpu);
+	return dl->locked;
+}
+
+extern unsigned int get_rq_info(void);
+
+static unsigned int state = MSM_MPDEC_DISABLED;
+
+static int get_slowest_cpu(void) {
+	unsigned int cpu, slow_cpu = 0, rate, slow_rate = 0;
+
+	for_each_online_cpu(cpu) {
+		if (cpu == 0)
+			continue;
+		rate = cpufreq_quick_get(cpu);
+		if (rate > 0 && slow_rate <= rate) {
+			slow_rate = rate;
+			slow_cpu = cpu;
+		}
+	}
+
+	return slow_cpu;
+}
+
+static unsigned int get_slowest_cpu_rate(void) {
+	unsigned int cpu, rate, slow_rate = 0;
+
+	for_each_online_cpu(cpu) {
+		rate = cpufreq_quick_get(cpu);
+		if (rate > 0 && slow_rate <= rate)
+			slow_rate = rate;
+	}
+
+	return slow_rate;
+}
+
+static int mp_decision(void) {
+	static bool first_call = true;
+	int new_state = MSM_MPDEC_IDLE;
+	int nr_cpu_online;
+	int index;
+	unsigned int rq_depth;
+	static cputime64_t total_time = 0;
+	static cputime64_t last_time;
+	cputime64_t current_time;
+	cputime64_t this_time = 0;
+
+	if (!hotplug.bricked_enabled)
+		return MSM_MPDEC_DISABLED;
+
+	current_time = ktime_to_ms(ktime_get());
+
+	if (first_call) {
+		first_call = false;
+	} else {
+		this_time = current_time - last_time;
+	}
+	total_time += this_time;
+
+	rq_depth = get_rq_info();
+	nr_cpu_online = num_online_cpus();
+
+	index = (nr_cpu_online - 1) * 2;
+	if ((nr_cpu_online < DEFAULT_MAX_CPUS_ONLINE) && (rq_depth >= NwNs_Threshold[index])) {
+		if ((total_time >= TwTs_Threshold[index]) &&
+			(nr_cpu_online < hotplug.max_cpus_online)) {
+			new_state = MSM_MPDEC_UP;
+			if (get_slowest_cpu_rate() <=  hotplug.idle_freq)
+				new_state = MSM_MPDEC_IDLE;
+		}
+	} else if ((nr_cpu_online > 1) && (rq_depth <= NwNs_Threshold[index+1])) {
+		if ((total_time >= TwTs_Threshold[index+1]) &&
+			(nr_cpu_online > hotplug.min_cpus_online)) {
+			new_state = MSM_MPDEC_DOWN;
+			if (get_slowest_cpu_rate() > hotplug.idle_freq)
+				new_state = MSM_MPDEC_IDLE;
+		}
+	} else {
+		new_state = MSM_MPDEC_IDLE;
+		total_time = 0;
+	}
+
+	if (new_state != MSM_MPDEC_IDLE) {
+		total_time = 0;
+	}
+
+	last_time = ktime_to_ms(ktime_get());
+#if DEBUG
+	pr_info(MPDEC_TAG"[DEBUG] rq: %u, new_state: %i | Mask=[%d%d%d%d]\n",
+			rq_depth, new_state, cpu_online(0), cpu_online(1), cpu_online(2), cpu_online(3));
+#endif
+	return new_state;
+}
+
+static void __ref bricked_hotplug_work(struct work_struct *work) {
+	unsigned int cpu;
+
+	if (hotplug.suspended && hotplug.max_cpus_online_susp <= 1)
+		goto out;
+
+	if (!mutex_trylock(&hotplug.bricked_cpu_mutex))
+		goto out;
+
+	state = mp_decision();
+	switch (state) {
+	case MSM_MPDEC_DISABLED:
+	case MSM_MPDEC_IDLE:
+		break;
+	case MSM_MPDEC_DOWN:
+		cpu = get_slowest_cpu();
+		if (cpu > 0) {
+			if (cpu_online(cpu) && !check_down_lock(cpu))
+				cpu_down(cpu);
+		}
+		break;
+	case MSM_MPDEC_UP:
+		cpu = cpumask_next_zero(0, cpu_online_mask);
+		if (cpu < DEFAULT_MAX_CPUS_ONLINE) {
+			if (!cpu_online(cpu)) {
+				cpu_up(cpu);
+				apply_down_lock(cpu);
+			}
+		}
+		break;
+	default:
+		pr_err(MPDEC_TAG": %s: invalid mpdec hotplug state %d\n",
+			__func__, state);
+	}
+	mutex_unlock(&hotplug.bricked_cpu_mutex);
+
+out:
+	if (hotplug.bricked_enabled)
+		queue_delayed_work(hotplug_wq, &hotplug_work,
+					msecs_to_jiffies(hotplug.delay));
+	return;
+}
+
+static void bricked_hotplug_suspend(struct work_struct *work)
+{
+	int cpu;
+
+	if (!hotplug.bricked_enabled || hotplug.suspended)
+		return;
+
+	if (!hotplug.hotplug_suspend)
+		return;
+
+	mutex_lock(&hotplug.bricked_hotplug_mutex);
+	hotplug.suspended = 1;
+	hotplug.min_cpus_online_res = hotplug.min_cpus_online;
+	hotplug.min_cpus_online = 1;
+	hotplug.max_cpus_online_res = hotplug.max_cpus_online;
+	hotplug.max_cpus_online = hotplug.max_cpus_online_susp;
+	mutex_unlock(&hotplug.bricked_hotplug_mutex);
+
+	if (hotplug.max_cpus_online_susp > 1) {
+		pr_info(MPDEC_TAG": Screen -> off\n");
+		return;
+	}
+
+	/* main work thread can sleep now */
+	cancel_delayed_work_sync(&hotplug_work);
+
+	for_each_possible_cpu(cpu) {
+		if ((cpu >= 1) && (cpu_online(cpu)))
+			cpu_down(cpu);
+	}
+
+	pr_info(MPDEC_TAG": Screen -> off. Deactivated bricked hotplug. | Mask=[%d%d%d%d]\n",
+			cpu_online(0), cpu_online(1), cpu_online(2), cpu_online(3));
+}
+
+static void __ref bricked_hotplug_resume(struct work_struct *work)
+{
+	int cpu, required_reschedule = 0, required_wakeup = 0;
+
+	if (!hotplug.bricked_enabled)
+		return;
+
+	if (!hotplug.hotplug_suspend)
+		return;
+
+	if (hotplug.suspended) {
+		mutex_lock(&hotplug.bricked_hotplug_mutex);
+		hotplug.suspended = 0;
+		hotplug.min_cpus_online = hotplug.min_cpus_online_res;
+		hotplug.max_cpus_online = hotplug.max_cpus_online_res;
+		mutex_unlock(&hotplug.bricked_hotplug_mutex);
+		required_wakeup = 1;
+		/* Initiate hotplug work if it was cancelled */
+		if (hotplug.max_cpus_online_susp <= 1) {
+			required_reschedule = 1;
+			INIT_DELAYED_WORK(&hotplug_work, bricked_hotplug_work);
+		}
+	}
+
+	if (required_wakeup) {
+		/* Fire up all CPUs */
+		for_each_cpu_not(cpu, cpu_online_mask) {
+			if (cpu == 0)
+				continue;
+			cpu_up(cpu);
+			apply_down_lock(cpu);
+		}
+	}
+
+	/* Resume hotplug workqueue if required */
+	if (required_reschedule) {
+		queue_delayed_work(hotplug_wq, &hotplug_work, 0);
+		pr_info(MPDEC_TAG": Screen -> on. Activated bricked hotplug. | Mask=[%d%d%d%d]\n",
+				cpu_online(0), cpu_online(1), cpu_online(2), cpu_online(3));
+	}
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __bricked_hotplug_suspend(struct power_suspend *handler)
+{
+	INIT_DELAYED_WORK(&suspend_work, bricked_hotplug_suspend);
+	mod_delayed_work_on(0, susp_wq, &suspend_work,
+			msecs_to_jiffies(hotplug.suspend_defer_time * 1000));
+}
+
+static void __ref __bricked_hotplug_resume(struct power_suspend *handler)
+{
+	flush_workqueue(susp_wq);
+	cancel_delayed_work_sync(&suspend_work);
+	queue_work_on(0, susp_wq, &resume_work);
+}
+
+static struct power_suspend bricked_hotplug_power_suspend_driver = {
+	.suspend = __bricked_hotplug_suspend,
+	.resume = __bricked_hotplug_resume,
+};
+#else
+static int prev_fb = FB_BLANK_UNBLANK;
+
+static int fb_notifier_callback(struct notifier_block *self,
+				unsigned long event, void *data)
+{
+	struct fb_event *evdata = data;
+	int *blank;
+
+	if (!hotplug.hotplug_suspend)
+		return NOTIFY_OK;
+
+	if (evdata && evdata->data && event == FB_EVENT_BLANK) {
+		blank = evdata->data;
+		switch (*blank) {
+			case FB_BLANK_UNBLANK:
+				if (prev_fb == FB_BLANK_POWERDOWN) {
+					/* display on */
+					flush_workqueue(susp_wq);
+					cancel_delayed_work_sync(&suspend_work);
+					queue_work_on(0, susp_wq, &resume_work);
+					prev_fb = FB_BLANK_UNBLANK;
+				}
+				break;
+			case FB_BLANK_POWERDOWN:
+				if (prev_fb == FB_BLANK_UNBLANK) {
+					/* display off */
+					INIT_DELAYED_WORK(&suspend_work, bricked_hotplug_suspend);
+					mod_delayed_work_on(0, susp_wq, &suspend_work,
+						msecs_to_jiffies(hotplug.suspend_defer_time * 1000));
+					prev_fb = FB_BLANK_POWERDOWN;
+				}
+				break;
+		}
+	}
+
+	return NOTIFY_OK;
+}
+#endif
+
+static int bricked_hotplug_start(void)
+{
+	int cpu, ret = 0;
+	struct down_lock *dl;
+
+	hotplug_wq = alloc_workqueue("bricked_hotplug", WQ_HIGHPRI | WQ_FREEZABLE, 0);
+	if (!hotplug_wq) {
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+	susp_wq =
+	    alloc_workqueue("susp_wq", WQ_FREEZABLE, 0);
+	if (!susp_wq) {
+		pr_err("%s: Failed to allocate suspend workqueue\n",
+		       MPDEC_TAG);
+		ret = -ENOMEM;
+		goto err_dev;
+	}
+
+#ifdef CONFIG_POWERSUSPEND
+	register_power_suspend(&bricked_hotplug_power_suspend_driver);
+#else
+	notif.notifier_call = fb_notifier_callback;
+	if (fb_register_client(&notif)) {
+		pr_err("%s: Failed to register FB notifier callback\n",
+			MPDEC_TAG);
+		goto err_susp;
+	}
+#endif
+
+	mutex_init(&hotplug.bricked_cpu_mutex);
+	mutex_init(&hotplug.bricked_hotplug_mutex);
+
+	INIT_DELAYED_WORK(&hotplug_work, bricked_hotplug_work);
+	INIT_DELAYED_WORK(&suspend_work, bricked_hotplug_suspend);
+	INIT_WORK(&resume_work, bricked_hotplug_resume);
+
+	for_each_possible_cpu(cpu) {
+		dl = &per_cpu(lock_info, cpu);
+		INIT_DELAYED_WORK(&dl->lock_rem, remove_down_lock);
+	}
+
+	if (hotplug.bricked_enabled)
+		queue_delayed_work(hotplug_wq, &hotplug_work,
+					msecs_to_jiffies(hotplug.startdelay));
+
+	return ret;
+#ifndef CONFIG_POWERSUSPEND
+err_susp:
+	destroy_workqueue(susp_wq);
+#endif
+err_dev:
+	destroy_workqueue(hotplug_wq);
+err_out:
+	hotplug.bricked_enabled = 0;
+	return ret;
+}
+
+static void bricked_hotplug_stop(void)
+{
+	int cpu;
+	struct down_lock *dl;
+
+	for_each_possible_cpu(cpu) {
+		dl = &per_cpu(lock_info, cpu);
+		cancel_delayed_work_sync(&dl->lock_rem);
+	}
+
+	flush_workqueue(susp_wq);
+	cancel_work_sync(&resume_work);
+	cancel_delayed_work_sync(&suspend_work);
+	cancel_delayed_work_sync(&hotplug_work);
+	mutex_destroy(&hotplug.bricked_hotplug_mutex);
+	mutex_destroy(&hotplug.bricked_cpu_mutex);
+#ifdef CONFIG_POWERSUSPEND
+	unregister_power_suspend(&bricked_hotplug_power_suspend_driver);
+#else
+	fb_unregister_client(&notif);
+	notif.notifier_call = NULL;
+#endif
+	destroy_workqueue(susp_wq);
+	destroy_workqueue(hotplug_wq);
+
+	/* Put all sibling cores to sleep */
+	for_each_online_cpu(cpu) {
+		if (cpu == 0)
+			continue;
+		cpu_down(cpu);
+	}
+}
+
+/**************************** SYSFS START ****************************/
+
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct device *dev, struct device_attribute *bricked_hotplug_attrs,	\
+ char *buf)								\
+{									\
+	return sprintf(buf, "%u\n", hotplug.object);			\
+}
+
+show_one(startdelay, startdelay);
+show_one(delay, delay);
+show_one(down_lock_duration, down_lock_dur);
+show_one(min_cpus_online, min_cpus_online);
+show_one(max_cpus_online, max_cpus_online);
+show_one(max_cpus_online_susp, max_cpus_online_susp);
+show_one(suspend_defer_time, suspend_defer_time);
+show_one(bricked_enabled, bricked_enabled);
+show_one(hotplug_suspend, hotplug_suspend);
+
+#define define_one_twts(file_name, arraypos)				\
+static ssize_t show_##file_name						\
+(struct device *dev, struct device_attribute *bricked_hotplug_attrs,	\
+ char *buf)								\
+{									\
+	return sprintf(buf, "%u\n", TwTs_Threshold[arraypos]);		\
+}									\
+static ssize_t store_##file_name					\
+(struct device *dev, struct device_attribute *bricked_hotplug_attrs,	\
+ const char *buf, size_t count)						\
+{									\
+	unsigned int input;						\
+	int ret;							\
+	ret = sscanf(buf, "%u", &input);				\
+	if (ret != 1)							\
+		return -EINVAL;						\
+	TwTs_Threshold[arraypos] = input;				\
+	return count;							\
+}									\
+static DEVICE_ATTR(file_name, 644, show_##file_name, store_##file_name);
+define_one_twts(twts_threshold_0, 0);
+define_one_twts(twts_threshold_1, 1);
+define_one_twts(twts_threshold_2, 2);
+define_one_twts(twts_threshold_3, 3);
+define_one_twts(twts_threshold_4, 4);
+define_one_twts(twts_threshold_5, 5);
+define_one_twts(twts_threshold_6, 6);
+define_one_twts(twts_threshold_7, 7);
+
+#define define_one_nwns(file_name, arraypos)				\
+static ssize_t show_##file_name						\
+(struct device *dev, struct device_attribute *bricked_hotplug_attrs,	\
+ char *buf)								\
+{									\
+	return sprintf(buf, "%u\n", NwNs_Threshold[arraypos]);		\
+}									\
+static ssize_t store_##file_name					\
+(struct device *dev, struct device_attribute *bricked_hotplug_attrs,	\
+ const char *buf, size_t count)						\
+{									\
+	unsigned int input;						\
+	int ret;							\
+	ret = sscanf(buf, "%u", &input);				\
+	if (ret != 1)							\
+		return -EINVAL;						\
+	NwNs_Threshold[arraypos] = input;				\
+	return count;							\
+}									\
+static DEVICE_ATTR(file_name, 644, show_##file_name, store_##file_name);
+define_one_nwns(nwns_threshold_0, 0);
+define_one_nwns(nwns_threshold_1, 1);
+define_one_nwns(nwns_threshold_2, 2);
+define_one_nwns(nwns_threshold_3, 3);
+define_one_nwns(nwns_threshold_4, 4);
+define_one_nwns(nwns_threshold_5, 5);
+define_one_nwns(nwns_threshold_6, 6);
+define_one_nwns(nwns_threshold_7, 7);
+
+static ssize_t show_idle_freq (struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				char *buf)
+{
+	return sprintf(buf, "%lu\n", hotplug.idle_freq);
+}
+
+static ssize_t store_startdelay(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.startdelay = input;
+
+	return count;
+}
+
+static ssize_t store_delay(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.delay = input;
+
+	return count;
+}
+
+static ssize_t store_down_lock_duration(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.down_lock_dur = val;
+
+	return count;
+}
+
+static ssize_t store_idle_freq(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	long unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%lu", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.idle_freq = input;
+
+	return count;
+}
+
+static ssize_t __ref store_min_cpus_online(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	unsigned int input, cpu;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if ((ret != 1) || input < 1 || input > DEFAULT_MAX_CPUS_ONLINE)
+		return -EINVAL;
+
+	if (hotplug.max_cpus_online < input)
+		hotplug.max_cpus_online = input;
+
+	hotplug.min_cpus_online = input;
+
+	if (!hotplug.bricked_enabled)
+		return count;
+
+	if (num_online_cpus() < hotplug.min_cpus_online) {
+		for (cpu = 1; cpu < DEFAULT_MAX_CPUS_ONLINE; cpu++) {
+			if (num_online_cpus() >= hotplug.min_cpus_online)
+				break;
+			if (cpu_online(cpu))
+				continue;
+			cpu_up(cpu);
+		}
+		pr_info(MPDEC_TAG": min_cpus_online set to %u. Affected CPUs were hotplugged!\n", input);
+	}
+
+	return count;
+}
+
+static ssize_t store_max_cpus_online(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	unsigned int input, cpu;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if ((ret != 1) || input < 1 || input > DEFAULT_MAX_CPUS_ONLINE)
+			return -EINVAL;
+
+	if (hotplug.min_cpus_online > input)
+		hotplug.min_cpus_online = input;
+
+	hotplug.max_cpus_online = input;
+
+	if (!hotplug.bricked_enabled)
+		return count;
+
+	if (num_online_cpus() > hotplug.max_cpus_online) {
+		for (cpu = DEFAULT_MAX_CPUS_ONLINE; cpu > 0; cpu--) {
+			if (num_online_cpus() <= hotplug.max_cpus_online)
+				break;
+			if (!cpu_online(cpu))
+				continue;
+			cpu_down(cpu);
+		}
+		pr_info(MPDEC_TAG": max_cpus set to %u. Affected CPUs were unplugged!\n", input);
+	}
+
+	return count;
+}
+
+static ssize_t store_max_cpus_online_susp(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if ((ret != 1) || input < 1 || input > DEFAULT_MAX_CPUS_ONLINE)
+			return -EINVAL;
+
+	hotplug.max_cpus_online_susp = input;
+
+	return count;
+}
+
+static ssize_t store_suspend_defer_time(struct device *dev,
+				    struct device_attribute *bricked_hotplug_attrs,
+				    const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.suspend_defer_time = val;
+
+	return count;
+}
+
+static ssize_t store_bricked_enabled(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == hotplug.bricked_enabled)
+		return count;
+
+	hotplug.bricked_enabled = input;
+
+	if (!hotplug.bricked_enabled) {
+		state = MSM_MPDEC_DISABLED;
+		bricked_hotplug_stop();
+		pr_info(MPDEC_TAG": Disabled\n");
+	} else {
+		state = MSM_MPDEC_IDLE;
+		bricked_hotplug_start();
+		pr_info(MPDEC_TAG": Enabled\n");
+	}
+
+	return count;
+}
+
+static ssize_t store_hotplug_suspend(struct device *dev,
+				struct device_attribute *bricked_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == hotplug.hotplug_suspend)
+		return count;
+
+	hotplug.hotplug_suspend = input;
+
+	return count;
+}
+
+static DEVICE_ATTR(startdelay, 644, show_startdelay, store_startdelay);
+static DEVICE_ATTR(delay, 644, show_delay, store_delay);
+static DEVICE_ATTR(down_lock_duration, 644, show_down_lock_duration, store_down_lock_duration);
+static DEVICE_ATTR(idle_freq, 644, show_idle_freq, store_idle_freq);
+static DEVICE_ATTR(min_cpus, 644, show_min_cpus_online, store_min_cpus_online);
+static DEVICE_ATTR(max_cpus, 644, show_max_cpus_online, store_max_cpus_online);
+static DEVICE_ATTR(min_cpus_online, 644, show_min_cpus_online, store_min_cpus_online);
+static DEVICE_ATTR(max_cpus_online, 644, show_max_cpus_online, store_max_cpus_online);
+static DEVICE_ATTR(max_cpus_online_susp, 644, show_max_cpus_online_susp, store_max_cpus_online_susp);
+static DEVICE_ATTR(suspend_defer_time, 644, show_suspend_defer_time, store_suspend_defer_time);
+static DEVICE_ATTR(enabled, 644, show_bricked_enabled, store_bricked_enabled);
+static DEVICE_ATTR(hotplug_suspend, 644, show_hotplug_suspend, store_hotplug_suspend);
+
+static struct attribute *bricked_hotplug_attrs[] = {
+	&dev_attr_startdelay.attr,
+	&dev_attr_delay.attr,
+	&dev_attr_down_lock_duration.attr,
+	&dev_attr_idle_freq.attr,
+	&dev_attr_min_cpus.attr,
+	&dev_attr_max_cpus.attr,
+	&dev_attr_min_cpus_online.attr,
+	&dev_attr_max_cpus_online.attr,
+	&dev_attr_max_cpus_online_susp.attr,
+	&dev_attr_suspend_defer_time.attr,
+	&dev_attr_enabled.attr,
+	&dev_attr_hotplug_suspend.attr,
+	&dev_attr_twts_threshold_0.attr,
+	&dev_attr_twts_threshold_1.attr,
+	&dev_attr_twts_threshold_2.attr,
+	&dev_attr_twts_threshold_3.attr,
+	&dev_attr_twts_threshold_4.attr,
+	&dev_attr_twts_threshold_5.attr,
+	&dev_attr_twts_threshold_6.attr,
+	&dev_attr_twts_threshold_7.attr,
+	&dev_attr_nwns_threshold_0.attr,
+	&dev_attr_nwns_threshold_1.attr,
+	&dev_attr_nwns_threshold_2.attr,
+	&dev_attr_nwns_threshold_3.attr,
+	&dev_attr_nwns_threshold_4.attr,
+	&dev_attr_nwns_threshold_5.attr,
+	&dev_attr_nwns_threshold_6.attr,
+	&dev_attr_nwns_threshold_7.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = bricked_hotplug_attrs,
+	.name = "conf",
+};
+
+/**************************** SYSFS END ****************************/
+
+static int bricked_hotplug_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct kobject *bricked_kobj;
+
+	bricked_kobj =
+		kobject_create_and_add("msm_mpdecision", kernel_kobj);
+	if (!bricked_kobj) {
+		pr_err("%s kobject create failed!\n",
+			__func__);
+		return -ENOMEM;
+        }
+
+	ret = sysfs_create_group(bricked_kobj,
+			&attr_group);
+
+        if (ret) {
+		pr_err("%s bricked_kobj create failed!\n",
+			__func__);
+		goto err_dev;
+	}
+
+	if (hotplug.bricked_enabled) {
+		ret = bricked_hotplug_start();
+		if (ret != 0)
+			goto err_dev;
+	}
+
+	return ret;
+err_dev:
+	if (bricked_kobj != NULL)
+		kobject_put(bricked_kobj);
+	return ret;
+}
+
+static struct platform_device bricked_hotplug_device = {
+	.name = MPDEC_TAG,
+	.id = -1,
+};
+
+static int bricked_hotplug_remove(struct platform_device *pdev)
+{
+	if (hotplug.bricked_enabled)
+		bricked_hotplug_stop();
+
+	return 0;
+}
+
+static struct platform_driver bricked_hotplug_driver = {
+	.probe = bricked_hotplug_probe,
+	.remove = bricked_hotplug_remove,
+	.driver = {
+		.name = MPDEC_TAG,
+		.owner = THIS_MODULE,
+	},
+};
+
+static int __init msm_mpdec_init(void)
+{
+	int ret = 0;
+
+	ret = platform_driver_register(&bricked_hotplug_driver);
+	if (ret) {
+		pr_err("%s: Driver register failed: %d\n", MPDEC_TAG, ret);
+		return ret;
+	}
+
+	ret = platform_device_register(&bricked_hotplug_device);
+	if (ret) {
+		pr_err("%s: Device register failed: %d\n", MPDEC_TAG, ret);
+		return ret;
+	}
+
+	pr_info(MPDEC_TAG": %s init complete.", __func__);
+
+	return ret;
+}
+
+void msm_mpdec_exit(void)
+{
+	platform_device_unregister(&bricked_hotplug_device);
+	platform_driver_unregister(&bricked_hotplug_driver);
+}
+
+late_initcall(msm_mpdec_init);
+module_exit(msm_mpdec_exit);
+
+MODULE_AUTHOR("Pranav Vashi <neobuddy89@gmail.com>");
+MODULE_DESCRIPTION("Bricked Hotplug Driver");
+MODULE_LICENSE("GPLv2");
diff --git a/arch/arm/mach-msm/cpufreq_limit.c b/arch/arm/mach-msm/cpufreq_limit.c
new file mode 100644
index 0000000..f51c8d5
--- /dev/null
+++ b/arch/arm/mach-msm/cpufreq_limit.c
@@ -0,0 +1,498 @@
+/*
+ * MSM CPU Frequency Limiter Driver
+ *
+ * Copyright (c) 2012-2014, Paul Reioux Faux123 <reioux@gmail.com>
+ * Copyright (c) 2014, Dorimanx <yuri@bynet.co.il>
+ * Copyright (c) 2014, Pranav Vashi <neobuddy89@gmail.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kobject.h>
+#include <linux/sysfs.h>
+#include <linux/workqueue.h>
+#include <linux/cpu.h>
+#include <linux/cpufreq.h>
+#include <mach/cpufreq.h>
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#else
+#include <linux/fb.h>
+#endif
+
+#define MSM_CPUFREQ_LIMIT_MAJOR		5
+#define MSM_CPUFREQ_LIMIT_MINOR		0
+
+#define MSM_LIMIT			"msm_cpufreq_limit"
+
+static struct workqueue_struct *limiter_wq;
+
+#define DEFAULT_SUSPEND_DEFER_TIME	10
+#define DEFAULT_SUSPEND_FREQUENCY	0
+#define DEFAULT_RESUME_FREQUENCY	2265600
+
+static unsigned int debug = 1;
+module_param_named(debug_mask, debug, uint, 0644);
+
+#define dprintk(msg...)		\
+do {				\
+	if (debug)		\
+		pr_info(msg);	\
+} while (0)
+
+static struct cpu_limit {
+	uint32_t suspend_max_freq;
+	uint32_t resume_max_freq;
+	unsigned int suspended;
+	unsigned int suspend_defer_time;
+	struct delayed_work suspend_work;
+	struct work_struct resume_work;
+	struct mutex msm_limiter_mutex;
+#ifndef CONFIG_POWERSUSPEND
+	struct notifier_block notif;
+#endif
+} limit = {
+	.suspend_max_freq = DEFAULT_SUSPEND_FREQUENCY,
+	.resume_max_freq = DEFAULT_RESUME_FREQUENCY,
+	.suspended = 0,
+	.suspend_defer_time = DEFAULT_SUSPEND_DEFER_TIME,
+};
+
+static void msm_limit_suspend(struct work_struct *work)
+{
+	/* Save current instance */
+	limit.resume_max_freq = get_max_lock(0);
+
+	if (!limit.resume_max_freq || limit.suspended)
+		return;
+
+	mutex_lock(&limit.msm_limiter_mutex);
+	limit.suspended = 1;
+	mutex_unlock(&limit.msm_limiter_mutex);
+
+	set_max_lock(0, limit.suspend_max_freq);
+
+	dprintk("Limit cpu0 max freq to %d\n",
+		limit.suspend_max_freq);
+}
+
+static void __ref msm_limit_resume(struct work_struct *work)
+{
+	/* Do not resume if didnt suspended */
+	if (!limit.suspended)
+		return;
+
+	mutex_lock(&limit.msm_limiter_mutex);
+	limit.suspended = 0;
+	mutex_unlock(&limit.msm_limiter_mutex);
+
+	set_max_lock(0, limit.resume_max_freq);
+
+	dprintk("Restore cpu0 max freq to %d\n",
+		limit.resume_max_freq);
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __msm_limit_suspend(struct power_suspend *handler)
+#else
+static void __msm_limit_suspend(void)
+#endif
+{
+	/* Do not suspend if suspend freq is not available */
+	if (limit.suspend_max_freq == 0)
+		return;
+
+	INIT_DELAYED_WORK(&limit.suspend_work, msm_limit_suspend);
+	mod_delayed_work_on(0, limiter_wq, &limit.suspend_work,
+			msecs_to_jiffies(limit.suspend_defer_time * 1000));
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __ref __msm_limit_resume(struct power_suspend *handler)
+#else
+static void __ref __msm_limit_resume(void)
+#endif
+{
+	/* Do not resume if suspend freq is not available */
+	if (limit.suspend_max_freq == 0)
+		return;
+
+	flush_workqueue(limiter_wq);
+	cancel_delayed_work_sync(&limit.suspend_work);
+	queue_work_on(0, limiter_wq, &limit.resume_work);
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static struct power_suspend msm_limit_power_suspend_driver = {
+	.suspend = __msm_limit_suspend,
+	.resume = __msm_limit_resume,
+};
+#else
+static int prev_fb = FB_BLANK_UNBLANK;
+
+static int fb_notifier_callback(struct notifier_block *self,
+				unsigned long event, void *data)
+{
+	struct fb_event *evdata = data;
+	int *blank;
+
+	if (evdata && evdata->data && event == FB_EVENT_BLANK) {
+		blank = evdata->data;
+		switch (*blank) {
+			case FB_BLANK_UNBLANK:
+				if (prev_fb == FB_BLANK_POWERDOWN) {
+					/* display on */
+					__msm_limit_resume();
+					prev_fb = FB_BLANK_UNBLANK;
+				}
+				break;
+			case FB_BLANK_POWERDOWN:
+				if (prev_fb == FB_BLANK_UNBLANK) {
+					/* display off */
+					__msm_limit_suspend();
+					prev_fb = FB_BLANK_POWERDOWN;
+				}
+				break;
+		}
+	}
+
+	return NOTIFY_OK;
+}
+#endif
+
+static int msm_cpufreq_limit_start(void)
+{
+	int ret = 0;
+
+	limiter_wq =
+	    alloc_workqueue("msm_limiter_wq", WQ_HIGHPRI | WQ_FREEZABLE, 0);
+	if (!limiter_wq) {
+		pr_err("%s: Failed to allocate limiter workqueue\n",
+		       MSM_LIMIT);
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+#ifdef CONFIG_POWERSUSPEND
+	register_power_suspend(&msm_limit_power_suspend_driver);
+#else
+	limit.notif.notifier_call = fb_notifier_callback;
+	if (fb_register_client(&limit.notif)) {
+		pr_err("%s: Failed to register FB notifier callback\n",
+			MSM_LIMIT);
+		goto err_dev;
+	}
+#endif
+
+	mutex_init(&limit.msm_limiter_mutex);
+	INIT_DELAYED_WORK(&limit.suspend_work, msm_limit_suspend);
+	INIT_WORK(&limit.resume_work, msm_limit_resume);
+
+	queue_work_on(0, limiter_wq, &limit.resume_work);
+
+	return ret;
+#ifndef CONFIG_POWERSUSPEND
+err_dev:
+	destroy_workqueue(limiter_wq);
+#endif
+err_out:
+	return ret;
+}
+
+static void msm_cpufreq_limit_stop(void)
+{
+	limit.suspended = 1;
+	flush_workqueue(limiter_wq);
+	cancel_work_sync(&limit.resume_work);
+	cancel_delayed_work_sync(&limit.suspend_work);
+	mutex_destroy(&limit.msm_limiter_mutex);
+#ifdef CONFIG_POWERSUSPEND
+	unregister_power_suspend(&msm_limit_power_suspend_driver);
+#else
+	fb_unregister_client(&limit.notif);
+	limit.notif.notifier_call = NULL;
+#endif
+	destroy_workqueue(limiter_wq);
+}
+
+static ssize_t suspend_defer_time_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", limit.suspend_defer_time);
+}
+
+static ssize_t suspend_defer_time_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u\n", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	limit.suspend_defer_time = val;
+
+	return count;
+}
+
+static ssize_t suspend_max_freq_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", limit.suspend_max_freq);
+}
+
+static ssize_t suspend_max_freq_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+	struct cpufreq_policy *policy = cpufreq_cpu_get(0);
+
+	ret = sscanf(buf, "%u\n", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (val == 0)
+		goto out;
+
+	if (val == limit.suspend_max_freq)
+		return count;
+
+	if (val < policy->cpuinfo.min_freq)
+		val = policy->cpuinfo.min_freq;
+	else if (val > policy->cpuinfo.max_freq)
+		val = policy->cpuinfo.max_freq;
+
+out:
+	limit.suspend_max_freq = val;
+
+	return count;
+}
+
+static ssize_t msm_cpufreq_limit_cpu0_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	unsigned int cpu = 0;
+	return sprintf(buf, "%u\n", get_max_lock(cpu));
+}
+
+static ssize_t msm_cpufreq_limit_cpu0_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	unsigned int cpu = 0;
+	unsigned int val;
+	int ret;
+
+	ret = sscanf(buf, "%u\n", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (val < 300000 || val > 2803200)
+		val = 0;
+
+	set_max_lock(cpu, val);
+
+	return count;
+}
+
+static ssize_t msm_cpufreq_limit_cpu1_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	unsigned int cpu = 1;
+	return sprintf(buf, "%u\n", get_max_lock(cpu));
+}
+
+static ssize_t msm_cpufreq_limit_cpu1_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	unsigned int cpu = 1;
+	unsigned int val;
+	int ret;
+
+	ret = sscanf(buf, "%u\n", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (val < 300000 || val > 2803200)
+		val = 0;
+
+	set_max_lock(cpu, val);
+
+	return count;
+}
+
+static ssize_t msm_cpufreq_limit_cpu2_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	unsigned int cpu = 2;
+	return sprintf(buf, "%u\n", get_max_lock(cpu));
+}
+
+static ssize_t msm_cpufreq_limit_cpu2_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	unsigned int cpu = 2;
+	unsigned int val;
+	int ret;
+
+	ret = sscanf(buf, "%u\n", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (val < 300000 || val > 2803200)
+		val = 0;
+
+	set_max_lock(cpu, val);
+
+	return count;
+}
+
+static ssize_t msm_cpufreq_limit_cpu3_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	unsigned int cpu = 3;
+	return sprintf(buf, "%u\n", get_max_lock(cpu));
+}
+
+static ssize_t msm_cpufreq_limit_cpu3_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	unsigned int cpu = 3;
+	unsigned int val;
+	int ret;
+
+	ret = sscanf(buf, "%u\n", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (val < 300000 || val > 2803200)
+		val = 0;
+
+	set_max_lock(cpu, val);
+
+	return count;
+}
+
+static ssize_t msm_cpufreq_limit_version_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "version: %u.%u\n",
+			MSM_CPUFREQ_LIMIT_MAJOR, MSM_CPUFREQ_LIMIT_MINOR);
+}
+
+static struct kobj_attribute msm_cpufreq_limit_cpu0_attribute =
+	__ATTR(cpufreq_limit_cpu0, 0666,
+		msm_cpufreq_limit_cpu0_show,
+		msm_cpufreq_limit_cpu0_store);
+
+static struct kobj_attribute msm_cpufreq_limit_cpu1_attribute =
+	__ATTR(cpufreq_limit_cpu1, 0666,
+		msm_cpufreq_limit_cpu1_show,
+		msm_cpufreq_limit_cpu1_store);
+
+static struct kobj_attribute msm_cpufreq_limit_cpu2_attribute =
+	__ATTR(cpufreq_limit_cpu2, 0666,
+		msm_cpufreq_limit_cpu2_show,
+		msm_cpufreq_limit_cpu2_store);
+
+static struct kobj_attribute msm_cpufreq_limit_cpu3_attribute =
+	__ATTR(cpufreq_limit_cpu3, 0666,
+		msm_cpufreq_limit_cpu3_show,
+		msm_cpufreq_limit_cpu3_store);
+
+static struct kobj_attribute msm_cpufreq_limit_version_attribute =
+	__ATTR(msm_cpufreq_limit_version, 0444,
+		msm_cpufreq_limit_version_show,
+		NULL);
+
+static struct kobj_attribute suspend_defer_time_attribute =
+	__ATTR(suspend_defer_time, 0666,
+		suspend_defer_time_show,
+		suspend_defer_time_store);
+
+static struct kobj_attribute suspend_max_freq_attribute =
+	__ATTR(suspend_max_freq, 0666,
+		suspend_max_freq_show,
+		suspend_max_freq_store);
+
+static struct attribute *msm_cpufreq_limit_attrs[] =
+	{
+		&msm_cpufreq_limit_cpu0_attribute.attr,
+		&msm_cpufreq_limit_cpu1_attribute.attr,
+		&msm_cpufreq_limit_cpu2_attribute.attr,
+		&msm_cpufreq_limit_cpu3_attribute.attr,
+		&msm_cpufreq_limit_version_attribute.attr,
+		&suspend_defer_time_attribute.attr,
+		&suspend_max_freq_attribute.attr,
+		NULL,
+	};
+
+static struct attribute_group msm_cpufreq_limit_attr_group =
+	{
+		.attrs = msm_cpufreq_limit_attrs,
+	};
+
+static struct kobject *msm_cpufreq_limit_kobj;
+
+static int msm_cpufreq_limit_init(void)
+{
+	int ret;
+
+	msm_cpufreq_limit_kobj =
+		kobject_create_and_add(MSM_LIMIT, kernel_kobj);
+	if (!msm_cpufreq_limit_kobj) {
+		pr_err("%s msm_cpufreq_limit_kobj kobject create failed!\n",
+			__func__);
+		return -ENOMEM;
+        }
+
+	ret = sysfs_create_group(msm_cpufreq_limit_kobj,
+			&msm_cpufreq_limit_attr_group);
+
+        if (ret) {
+		pr_err("%s msm_cpufreq_limit_kobj create failed!\n",
+			__func__);
+		goto err_dev;
+	}
+
+	msm_cpufreq_limit_start();
+
+	return ret;
+err_dev:
+	if (msm_cpufreq_limit_kobj != NULL)
+		kobject_put(msm_cpufreq_limit_kobj);
+	return ret;
+}
+
+static void msm_cpufreq_limit_exit(void)
+{
+	if (msm_cpufreq_limit_kobj != NULL)
+		kobject_put(msm_cpufreq_limit_kobj);
+
+	msm_cpufreq_limit_stop();
+}
+
+module_init(msm_cpufreq_limit_init);
+module_exit(msm_cpufreq_limit_exit);
+
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>, \
+		Dorimanx <yuri@bynet.co.il>, \
+		Pranav Vashi <neobuddy89@gmail.com>");
+MODULE_DESCRIPTION("MSM Krait CPU Frequency Limiter Driver");
+MODULE_LICENSE("GPL v2");
diff --git a/arch/arm/mach-msm/fastchg.c b/arch/arm/mach-msm/fastchg.c
new file mode 100644
index 0000000..bf697df
--- /dev/null
+++ b/arch/arm/mach-msm/fastchg.c
@@ -0,0 +1,211 @@
+/*
+ * based on sysfs interface from:
+ *	Chad Froebel <chadfroebel@gmail.com> &
+ *	Jean-Pierre Rasquin <yank555.lu@gmail.com>
+ * for backwards compatibility
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+/*
+ * Possible values for "force_fast_charge" are :
+ *
+ *   0 - disabled (default)
+ *   1 - substitute AC to USB unconditional
+ *   2 - custom
+*/
+
+#include <linux/module.h>
+#include <linux/kobject.h>
+#include <linux/sysfs.h>
+#include <linux/fastchg.h>
+
+/* Credits / Changelog:
+ * version 1.0 Initial build by Paul Reioux
+ * version 1.1 Added 1800ma limit to table by Dorimanx
+ * version 1.2 Added Fake AC interface by Mankindtw@xda and Dorimanx 
+ * (update 22/10/14 mod deleted, it's bugged and useless)
+ * version 1.3 Misc fixes to force AC and allowed real 1800mA max.
+ *
+ * Next versions depend on code for LG G2 Device!!! (Dorimanx)
+ * version 1.4 Added usage of custom mA value for max charging power,
+ * Now we can use Intelli Thermal and get full power charge, this was controlled by
+ * default ROM thermal engine, not any more, code will check if battery if not above 50c
+ * and allow max charge!
+ * version 1.5/6/7/8 trying to perfect fast charge auto on/off and auto tune based on connection type
+ * and battery heat.
+ * version 1.9 Added Auto fast charge on/off based on battery %, if above 95% then fast charge is OFF
+ * when battery is below 95% and fast charge was ON by user before, then it's enabled again.
+ * version 2.0 Guard with mutex all functions that use values from other code to prevent race and bug.
+ * version 2.1 Corect Mutex guards in code for fastcharge.
+ * version 2.2 allow to charge on 900ma lock.
+ * version 2.3 added more checks to thermal mitigation functions and corrected code style.
+ * removed updating charging scenario when no charger connected. no point to do so.
+ * version 2.4 allowed full 2000ma to be set in charger driver.
+ * version 2.5 fixed broken mitigation set if USB is connected.
+ */
+
+#define FAST_CHARGE_VERSION	"Version 2.5"
+
+int force_fast_charge;
+int force_fast_charge_temp;
+int fast_charge_level;
+int force_fast_charge_on_off;
+
+/* sysfs interface for "force_fast_charge" */
+static ssize_t force_fast_charge_show(struct kobject *kobj,
+			struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", force_fast_charge);
+}
+
+static ssize_t force_fast_charge_store(struct kobject *kobj,
+			struct kobj_attribute *attr, const char *buf,
+			size_t count)
+{
+
+	int new_force_fast_charge;
+
+	sscanf(buf, "%du", &new_force_fast_charge);
+
+	switch(new_force_fast_charge) {
+		case FAST_CHARGE_DISABLED:
+		case FAST_CHARGE_FORCE_AC:
+		case FAST_CHARGE_FORCE_CUSTOM_MA:
+			force_fast_charge = new_force_fast_charge;
+			force_fast_charge_temp = new_force_fast_charge;
+			force_fast_charge_on_off = new_force_fast_charge;
+			return count;
+		default:
+			return -EINVAL;
+	}
+}
+
+static ssize_t charge_level_show(struct kobject *kobj,
+				struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", fast_charge_level);
+}
+
+static ssize_t charge_level_store(struct kobject *kobj,
+			struct kobj_attribute *attr, const char *buf,
+			size_t count)
+{
+
+	int new_charge_level;
+
+	sscanf(buf, "%du", &new_charge_level);
+
+	switch (new_charge_level) {
+		case FAST_CHARGE_300:
+		case FAST_CHARGE_500:
+		case FAST_CHARGE_900:
+		case FAST_CHARGE_1200:
+		case FAST_CHARGE_1600:
+		case FAST_CHARGE_1800:
+		case FAST_CHARGE_2000:
+			fast_charge_level = new_charge_level;
+			return count;
+		default:
+			return -EINVAL;
+	}
+	return -EINVAL;
+}
+
+/* sysfs interface for "fast_charge_levels" */
+static ssize_t available_charge_levels_show(struct kobject *kobj,
+			struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%s\n", FAST_CHARGE_LEVELS);
+}
+
+/* sysfs interface for "version" */
+static ssize_t version_show(struct kobject *kobj,
+			struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%s\n", FAST_CHARGE_VERSION);
+}
+
+static struct kobj_attribute version_attribute =
+	__ATTR(version, 0444, version_show, NULL);
+
+static struct kobj_attribute available_charge_levels_attribute =
+	__ATTR(available_charge_levels, 0444,
+		available_charge_levels_show, NULL);
+
+static struct kobj_attribute fast_charge_level_attribute =
+	__ATTR(fast_charge_level, 0666,
+		charge_level_show,
+		charge_level_store);
+
+static struct kobj_attribute force_fast_charge_attribute =
+	__ATTR(force_fast_charge, 0666,
+		force_fast_charge_show,
+		force_fast_charge_store);
+
+static struct attribute *force_fast_charge_attrs[] = {
+	&force_fast_charge_attribute.attr,
+	&fast_charge_level_attribute.attr,
+	&available_charge_levels_attribute.attr,
+	&version_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group force_fast_charge_attr_group = {
+	.attrs = force_fast_charge_attrs,
+};
+
+/* Initialize fast charge sysfs folder */
+static struct kobject *force_fast_charge_kobj;
+
+int force_fast_charge_init(void)
+{
+	int force_fast_charge_retval;
+
+	 /* Forced fast charge disabled by default */
+	force_fast_charge = FAST_CHARGE_DISABLED;
+	force_fast_charge_temp = FAST_CHARGE_DISABLED;
+	force_fast_charge_on_off = FAST_CHARGE_DISABLED;
+	fast_charge_level = FAST_CHARGE_1600;
+
+	force_fast_charge_kobj
+		= kobject_create_and_add("fast_charge", kernel_kobj);
+
+	if (!force_fast_charge_kobj) {
+		return -ENOMEM;
+	}
+
+	force_fast_charge_retval
+		= sysfs_create_group(force_fast_charge_kobj,
+				&force_fast_charge_attr_group);
+
+	if (force_fast_charge_retval)
+		kobject_put(force_fast_charge_kobj);
+
+	if (force_fast_charge_retval)
+		kobject_put(force_fast_charge_kobj);
+
+	return (force_fast_charge_retval);
+}
+
+void force_fast_charge_exit(void)
+{
+	kobject_put(force_fast_charge_kobj);
+}
+
+module_init(force_fast_charge_init);
+module_exit(force_fast_charge_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Jean-Pierre Rasquin <yank555.lu@gmail.com>");
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>");
+MODULE_AUTHOR("Yuri Sh. <yuri@bynet.co.il>");
+MODULE_DESCRIPTION("Fast Charge Hack for Android");
diff --git a/arch/arm/mach-msm/intelli_hotplug.c b/arch/arm/mach-msm/intelli_hotplug.c
new file mode 100644
index 0000000..afa7a14
--- /dev/null
+++ b/arch/arm/mach-msm/intelli_hotplug.c
@@ -0,0 +1,851 @@
+/*
+ * Intelli Hotplug Driver
+ *
+ * Copyright (c) 2013-2014, Paul Reioux <reioux@gmail.com>
+ * Copyright (c) 2010-2014, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/workqueue.h>
+#include <linux/cpu.h>
+#include <linux/sched.h>
+#include <linux/mutex.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/input.h>
+#include <linux/kobject.h>
+#include <linux/cpufreq.h>
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#else
+#include <linux/fb.h>
+#endif
+
+#define INTELLI_PLUG			"intelli_plug"
+#define INTELLI_PLUG_MAJOR_VERSION	5
+#define INTELLI_PLUG_MINOR_VERSION	4
+
+#define DEF_SAMPLING_MS			30
+#define RESUME_SAMPLING_MS		HZ / 10
+#define START_DELAY_MS			HZ * 10
+#define MIN_INPUT_INTERVAL		150 * 1000L
+#define BOOST_LOCK_DUR			500 * 1000L
+#define DEFAULT_NR_CPUS_BOOSTED		2
+#define DEFAULT_MIN_CPUS_ONLINE		1
+#define DEFAULT_MAX_CPUS_ONLINE		NR_CPUS
+#define DEFAULT_NR_FSHIFT		DEFAULT_MAX_CPUS_ONLINE - 1
+#define DEFAULT_DOWN_LOCK_DUR		2000
+#define DEFAULT_SUSPEND_DEFER_TIME	10
+
+#define CAPACITY_RESERVE		50
+#if defined(CONFIG_ARCH_MSM8960) || defined(CONFIG_ARCH_APQ8064) || \
+defined(CONFIG_ARCH_MSM8974)
+#define THREAD_CAPACITY			(339 - CAPACITY_RESERVE)
+#elif defined(CONFIG_ARCH_MSM8226) || defined (CONFIG_ARCH_MSM8926) || \
+defined (CONFIG_ARCH_MSM8610) || defined (CONFIG_ARCH_MSM8228)
+#define THREAD_CAPACITY			(190 - CAPACITY_RESERVE)
+#else
+#define THREAD_CAPACITY			(250 - CAPACITY_RESERVE)
+#endif
+#define CPU_NR_THRESHOLD		((THREAD_CAPACITY << 1) + \
+					(THREAD_CAPACITY / 2))
+#define MULT_FACTOR			4
+#define DIV_FACTOR			100000
+
+static u64 last_boost_time, last_input;
+
+static struct delayed_work intelli_plug_work;
+static struct work_struct up_down_work;
+static struct workqueue_struct *intelliplug_wq;
+static struct workqueue_struct *susp_wq;
+static struct delayed_work suspend_work;
+static struct work_struct resume_work;
+static struct mutex intelli_plug_mutex;
+#ifndef CONFIG_POWERSUSPEND
+static struct notifier_block notif;
+#endif
+
+struct ip_cpu_info {
+	unsigned long cpu_nr_running;
+};
+static DEFINE_PER_CPU(struct ip_cpu_info, ip_info);
+
+/* HotPlug Driver controls */
+static atomic_t intelli_plug_active = ATOMIC_INIT(0);
+static unsigned int cpus_boosted = DEFAULT_NR_CPUS_BOOSTED;
+static unsigned int min_cpus_online = DEFAULT_MIN_CPUS_ONLINE;
+static unsigned int max_cpus_online = DEFAULT_MAX_CPUS_ONLINE;
+static unsigned int full_mode_profile = 0; /* balance profile */
+static unsigned int cpu_nr_run_threshold = CPU_NR_THRESHOLD;
+
+static bool hotplug_suspended = false;
+unsigned int suspend_defer_time = DEFAULT_SUSPEND_DEFER_TIME;
+static unsigned int min_cpus_online_res = DEFAULT_MIN_CPUS_ONLINE;
+static unsigned int max_cpus_online_res = DEFAULT_MAX_CPUS_ONLINE;
+/*
+ * suspend mode, if set = 1 hotplug will sleep,
+ * if set = 0, then hoplug will be active all the time.
+ */
+static unsigned int hotplug_suspend = 0;
+
+/* HotPlug Driver Tuning */
+static unsigned int target_cpus = 0;
+static u64 boost_lock_duration = BOOST_LOCK_DUR;
+static unsigned int def_sampling_ms = DEF_SAMPLING_MS;
+static unsigned int nr_fshift = DEFAULT_NR_FSHIFT;
+static unsigned int nr_run_hysteresis = DEFAULT_MAX_CPUS_ONLINE * 2;
+static unsigned int debug_intelli_plug = 1;
+
+#define dprintk(msg...)		\
+do {				\
+	if (debug_intelli_plug)		\
+		pr_info(msg);	\
+} while (0)
+
+static unsigned int nr_run_thresholds_balance[] = {
+	(THREAD_CAPACITY * 625 * MULT_FACTOR) / DIV_FACTOR,
+	(THREAD_CAPACITY * 875 * MULT_FACTOR) / DIV_FACTOR,
+	(THREAD_CAPACITY * 1125 * MULT_FACTOR) / DIV_FACTOR,
+	UINT_MAX
+};
+
+static unsigned int nr_run_thresholds_performance[] = {
+	(THREAD_CAPACITY * 380 * MULT_FACTOR) / DIV_FACTOR,
+	(THREAD_CAPACITY * 625 * MULT_FACTOR) / DIV_FACTOR,
+	(THREAD_CAPACITY * 875 * MULT_FACTOR) / DIV_FACTOR,
+	UINT_MAX
+};
+
+static unsigned int nr_run_thresholds_conservative[] = {
+	(THREAD_CAPACITY * 875 * MULT_FACTOR) / DIV_FACTOR,
+	(THREAD_CAPACITY * 1625 * MULT_FACTOR) / DIV_FACTOR,
+	(THREAD_CAPACITY * 2125 * MULT_FACTOR) / DIV_FACTOR,
+	UINT_MAX
+};
+
+static unsigned int nr_run_thresholds_disable[] = {
+	0,  0,  0,  UINT_MAX
+};
+
+static unsigned int nr_run_thresholds_tri[] = {
+	(THREAD_CAPACITY * 625 * MULT_FACTOR) / DIV_FACTOR,
+	(THREAD_CAPACITY * 875 * MULT_FACTOR) / DIV_FACTOR,
+	UINT_MAX
+};
+
+static unsigned int nr_run_thresholds_eco[] = {
+        (THREAD_CAPACITY * 380 * MULT_FACTOR) / DIV_FACTOR,
+	UINT_MAX
+};
+
+static unsigned int nr_run_thresholds_strict[] = {
+	UINT_MAX
+};
+
+static unsigned int *nr_run_profiles[] = {
+	nr_run_thresholds_balance,
+	nr_run_thresholds_performance,
+	nr_run_thresholds_conservative,
+	nr_run_thresholds_disable,
+	nr_run_thresholds_tri,
+	nr_run_thresholds_eco,
+	nr_run_thresholds_strict
+	};
+
+static unsigned int nr_run_last;
+static unsigned int down_lock_dur = DEFAULT_DOWN_LOCK_DUR;
+
+struct down_lock {
+	unsigned int locked;
+	struct delayed_work lock_rem;
+};
+static DEFINE_PER_CPU(struct down_lock, lock_info);
+
+static void apply_down_lock(unsigned int cpu)
+{
+	struct down_lock *dl = &per_cpu(lock_info, cpu);
+
+	dl->locked = 1;
+	mod_delayed_work_on(0, intelliplug_wq, &dl->lock_rem,
+			      msecs_to_jiffies(down_lock_dur));
+}
+
+static void remove_down_lock(struct work_struct *work)
+{
+	struct down_lock *dl = container_of(work, struct down_lock,
+					    lock_rem.work);
+	dl->locked = 0;
+}
+
+static int check_down_lock(unsigned int cpu)
+{
+	struct down_lock *dl = &per_cpu(lock_info, cpu);
+	return dl->locked;
+}
+
+static unsigned int calculate_thread_stats(void)
+{
+	unsigned int avg_nr_run = avg_nr_running();
+	unsigned int nr_run;
+	unsigned int threshold_size;
+	unsigned int *current_profile;
+
+	threshold_size = max_cpus_online;
+	nr_run_hysteresis = max_cpus_online * 2;
+	nr_fshift = max_cpus_online - 1;
+
+	for (nr_run = 1; nr_run < threshold_size; nr_run++) {
+		unsigned int nr_threshold;
+		if (max_cpus_online >= 4)
+			current_profile = nr_run_profiles[full_mode_profile];
+		else if (max_cpus_online == 3)
+			current_profile = nr_run_profiles[4];
+		else if (max_cpus_online == 2)
+			current_profile = nr_run_profiles[5];
+		else
+			current_profile = nr_run_profiles[6];
+
+		nr_threshold = current_profile[nr_run - 1];
+
+		if (nr_run_last <= nr_run)
+			nr_threshold += nr_run_hysteresis;
+		if (avg_nr_run <= (nr_threshold << (FSHIFT - nr_fshift)))
+			break;
+	}
+	nr_run_last = nr_run;
+
+	return nr_run;
+}
+
+static void update_per_cpu_stat(void)
+{
+	unsigned int cpu;
+	struct ip_cpu_info *l_ip_info;
+
+	for_each_online_cpu(cpu) {
+		l_ip_info = &per_cpu(ip_info, cpu);
+		l_ip_info->cpu_nr_running = avg_cpu_nr_running(cpu);
+	}
+}
+
+static void __ref cpu_up_down_work(struct work_struct *work)
+{
+	int online_cpus, cpu, l_nr_threshold;
+	int target = target_cpus;
+	struct ip_cpu_info *l_ip_info;
+
+	if (target < min_cpus_online)
+		target = min_cpus_online;
+	else if (target > max_cpus_online)
+		target = max_cpus_online;
+
+	online_cpus = num_online_cpus();
+
+	if (target < online_cpus) {
+		if (online_cpus <= cpus_boosted &&
+		    (ktime_to_us(ktime_get()) - last_input <
+				boost_lock_duration))
+			return;
+
+		update_per_cpu_stat();
+		for_each_online_cpu(cpu) {
+			if (cpu == 0)
+				continue;
+			if (check_down_lock(cpu))
+				break;
+			l_nr_threshold =
+				cpu_nr_run_threshold << 1 /
+					(num_online_cpus());
+			l_ip_info = &per_cpu(ip_info, cpu);
+			if (l_ip_info->cpu_nr_running < l_nr_threshold)
+				cpu_down(cpu);
+			if (target >= num_online_cpus())
+				break;
+		}
+	} else if (target > online_cpus) {
+		for_each_cpu_not(cpu, cpu_online_mask) {
+			if (cpu == 0)
+				continue;
+			cpu_up(cpu);
+			apply_down_lock(cpu);
+			if (target <= num_online_cpus())
+				break;
+		}
+	}
+}
+
+static void intelli_plug_work_fn(struct work_struct *work)
+{
+	if (hotplug_suspended) {
+		dprintk("intelli_plug is suspended!\n");
+		return;
+	}
+
+	target_cpus = calculate_thread_stats();
+	queue_work_on(0, intelliplug_wq, &up_down_work);
+
+	if (atomic_read(&intelli_plug_active) == 1)
+		mod_delayed_work_on(0, intelliplug_wq, &intelli_plug_work,
+					msecs_to_jiffies(def_sampling_ms));
+}
+
+static void intelli_plug_suspend(struct work_struct *work)
+{
+	if (hotplug_suspended == false) {
+		mutex_lock(&intelli_plug_mutex);
+		hotplug_suspended = true;
+		min_cpus_online_res = min_cpus_online;
+		min_cpus_online = 1;
+		max_cpus_online_res = max_cpus_online;
+		max_cpus_online = 1;
+		mutex_unlock(&intelli_plug_mutex);
+
+		/* Flush hotplug workqueue */
+		flush_workqueue(intelliplug_wq);
+		cancel_delayed_work_sync(&intelli_plug_work);
+
+		dprintk("%s: suspended!\n", INTELLI_PLUG);
+	}
+}
+
+static void __ref intelli_plug_resume(struct work_struct *work)
+{
+	int cpu, required_reschedule = 0, required_wakeup = 0;
+
+	if (hotplug_suspended) {
+		mutex_lock(&intelli_plug_mutex);
+		hotplug_suspended = false;
+		min_cpus_online = min_cpus_online_res;
+		max_cpus_online = max_cpus_online_res;
+		mutex_unlock(&intelli_plug_mutex);
+		required_wakeup = 1;
+		/* Initiate hotplug work if it was cancelled */
+		required_reschedule = 1;
+		INIT_DELAYED_WORK(&intelli_plug_work,
+				intelli_plug_work_fn);
+		dprintk("%s: resumed.\n", INTELLI_PLUG);
+	}
+
+	if (required_wakeup) {
+		/* Fire up all CPUs */
+		for_each_cpu_not(cpu, cpu_online_mask) {
+			if (cpu == 0)
+				continue;
+			cpu_up(cpu);
+			apply_down_lock(cpu);
+		}
+		dprintk("%s: wakeup boosted.\n", INTELLI_PLUG);
+	}
+
+	/* Resume hotplug workqueue if required */
+	if (required_reschedule)
+		mod_delayed_work_on(0, intelliplug_wq, &intelli_plug_work,
+				      msecs_to_jiffies(RESUME_SAMPLING_MS));
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __intelli_plug_suspend(struct power_suspend *handler)
+#else
+static void __intelli_plug_suspend(void)
+#endif
+{
+	if ((atomic_read(&intelli_plug_active) == 0) ||
+			hotplug_suspended)
+		return;
+
+	if (!hotplug_suspend)
+		return;
+
+	INIT_DELAYED_WORK(&suspend_work, intelli_plug_suspend);
+	mod_delayed_work_on(0, susp_wq, &suspend_work,
+				 msecs_to_jiffies(suspend_defer_time * 1000));
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __ref __intelli_plug_resume(struct power_suspend *handler)
+#else
+static void __ref __intelli_plug_resume(void)
+#endif
+{
+	int cpu;
+
+	if (atomic_read(&intelli_plug_active) == 0)
+		return;
+
+	if (!hotplug_suspend) {
+		/* Fire up all CPUs */
+		for_each_cpu_not(cpu, cpu_online_mask) {
+			if (cpu == 0)
+				continue;
+			cpu_up(cpu);
+			apply_down_lock(cpu);
+		}
+		dprintk("%s: wakeup boosted.\n", INTELLI_PLUG);
+
+		return;
+	}
+
+	flush_workqueue(susp_wq);
+	cancel_delayed_work_sync(&suspend_work);
+	queue_work_on(0, susp_wq, &resume_work);
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static struct power_suspend intelli_plug_power_suspend_driver = {
+	.suspend = __intelli_plug_suspend,
+	.resume = __intelli_plug_resume,
+};
+#else
+static int prev_fb = FB_BLANK_UNBLANK;
+
+static int fb_notifier_callback(struct notifier_block *self,
+				unsigned long event, void *data)
+{
+	struct fb_event *evdata = data;
+	int *blank;
+
+	if (evdata && evdata->data && event == FB_EVENT_BLANK) {
+		blank = evdata->data;
+		switch (*blank) {
+			case FB_BLANK_UNBLANK:
+				if (prev_fb == FB_BLANK_POWERDOWN) {
+					/* display on */
+					__intelli_plug_resume();
+					prev_fb = FB_BLANK_UNBLANK;
+				}
+				break;
+			case FB_BLANK_POWERDOWN:
+				if (prev_fb == FB_BLANK_UNBLANK) {
+					/* display off */
+					__intelli_plug_suspend();
+					prev_fb = FB_BLANK_POWERDOWN;
+				}
+				break;
+		}
+	}
+
+	return NOTIFY_OK;
+}
+#endif
+
+static void intelli_plug_input_event(struct input_handle *handle,
+		unsigned int type, unsigned int code, int value)
+{
+	u64 now;
+
+	if (hotplug_suspended)
+		return;
+
+	now = ktime_to_us(ktime_get());
+	last_input = now;
+
+	if (now - last_boost_time < MIN_INPUT_INTERVAL)
+		return;
+
+	if (num_online_cpus() >= cpus_boosted ||
+	    cpus_boosted <= min_cpus_online)
+		return;
+
+	target_cpus = cpus_boosted;
+	queue_work_on(0, intelliplug_wq, &up_down_work);
+	last_boost_time = ktime_to_us(ktime_get());
+}
+
+static int intelli_plug_input_connect(struct input_handler *handler,
+				 struct input_dev *dev,
+				 const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int err;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = handler->name;
+
+	err = input_register_handle(handle);
+	if (err)
+		goto err_register;
+
+	err = input_open_device(handle);
+	if (err)
+		goto err_open;
+
+	return 0;
+err_open:
+	input_unregister_handle(handle);
+err_register:
+	kfree(handle);
+	return err;
+}
+
+static void intelli_plug_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id intelli_plug_ids[] = {
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_MT_POSITION_X)] =
+			    BIT_MASK(ABS_MT_POSITION_X) |
+			    BIT_MASK(ABS_MT_POSITION_Y) },
+	}, /* multi-touch touchscreen */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_KEYBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.keybit = { [BIT_WORD(BTN_TOUCH)] = BIT_MASK(BTN_TOUCH) },
+		.absbit = { [BIT_WORD(ABS_X)] =
+			    BIT_MASK(ABS_X) | BIT_MASK(ABS_Y) },
+	}, /* touchpad */
+	{ },
+};
+
+static struct input_handler intelli_plug_input_handler = {
+	.event          = intelli_plug_input_event,
+	.connect        = intelli_plug_input_connect,
+	.disconnect     = intelli_plug_input_disconnect,
+	.name           = "intelliplug_handler",
+	.id_table       = intelli_plug_ids,
+};
+
+static int __ref intelli_plug_start(void)
+{
+	int cpu, ret = 0;
+	struct down_lock *dl;
+
+	intelliplug_wq = alloc_workqueue("intelliplug",
+			WQ_HIGHPRI | WQ_FREEZABLE, 0);
+	if (!intelliplug_wq) {
+		pr_err("%s: Failed to allocate hotplug workqueue\n",
+		       INTELLI_PLUG);
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+	susp_wq =
+	    alloc_workqueue("intelli_susp_wq", WQ_FREEZABLE, 0);
+	if (!susp_wq) {
+		pr_err("%s: Failed to allocate suspend workqueue\n",
+		       INTELLI_PLUG);
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+#ifdef CONFIG_POWERSUSPEND
+	register_power_suspend(&intelli_plug_power_suspend_driver);
+#else
+	notif.notifier_call = fb_notifier_callback;
+	if (fb_register_client(&notif)) {
+		pr_err("%s: Failed to register FB notifier callback\n",
+			INTELLI_PLUG);
+		goto err_dev;
+	}
+#endif
+
+	ret = input_register_handler(&intelli_plug_input_handler);
+	if (ret) {
+		pr_err("%s: Failed to register input handler: %d\n",
+			INTELLI_PLUG, ret);
+		goto err_dev;
+	}
+
+	mutex_init(&intelli_plug_mutex);
+
+	INIT_WORK(&up_down_work, cpu_up_down_work);
+	INIT_DELAYED_WORK(&intelli_plug_work, intelli_plug_work_fn);
+	for_each_possible_cpu(cpu) {
+		dl = &per_cpu(lock_info, cpu);
+		INIT_DELAYED_WORK(&dl->lock_rem, remove_down_lock);
+	}
+	INIT_DELAYED_WORK(&suspend_work, intelli_plug_suspend);
+	INIT_WORK(&resume_work, intelli_plug_resume);
+
+	/* Put all sibling cores to sleep */
+	for_each_online_cpu(cpu) {
+		if (cpu == 0)
+			continue;
+		cpu_down(cpu);
+	}
+
+	mod_delayed_work_on(0, intelliplug_wq, &intelli_plug_work,
+			      START_DELAY_MS);
+
+	return ret;
+err_dev:
+	destroy_workqueue(intelliplug_wq);
+err_out:
+	atomic_set(&intelli_plug_active, 0);
+	return ret;
+}
+
+static void intelli_plug_stop(void)
+{
+	int cpu;
+	struct down_lock *dl;
+
+	flush_workqueue(susp_wq);
+	cancel_work_sync(&resume_work);
+	cancel_delayed_work_sync(&suspend_work);
+
+	for_each_possible_cpu(cpu) {
+		dl = &per_cpu(lock_info, cpu);
+		cancel_delayed_work_sync(&dl->lock_rem);
+	}
+	flush_workqueue(intelliplug_wq);
+	cancel_work_sync(&up_down_work);
+	cancel_delayed_work_sync(&intelli_plug_work);
+	mutex_destroy(&intelli_plug_mutex);
+#ifdef CONFIG_POWERSUSPEND
+	unregister_power_suspend(&intelli_plug_power_suspend_driver);
+#else
+	fb_unregister_client(&notif);
+	notif.notifier_call = NULL;
+#endif
+
+	input_unregister_handler(&intelli_plug_input_handler);
+	destroy_workqueue(susp_wq);
+	destroy_workqueue(intelliplug_wq);
+}
+
+static void intelli_plug_active_eval_fn(unsigned int status)
+{
+	int ret = 0;
+
+	if (status == 1) {
+		ret = intelli_plug_start();
+		if (ret)
+			status = 0;
+	} else
+		intelli_plug_stop();
+
+	atomic_set(&intelli_plug_active, status);
+}
+
+#define show_one(file_name, object)				\
+static ssize_t show_##file_name					\
+(struct kobject *kobj, struct kobj_attribute *attr, char *buf)	\
+{								\
+	return sprintf(buf, "%u\n", object);			\
+}
+
+show_one(cpus_boosted, cpus_boosted);
+show_one(min_cpus_online, min_cpus_online);
+show_one(max_cpus_online, max_cpus_online);
+show_one(suspend_defer_time, suspend_defer_time);
+show_one(hotplug_suspend, hotplug_suspend);
+show_one(full_mode_profile, full_mode_profile);
+show_one(cpu_nr_run_threshold, cpu_nr_run_threshold);
+show_one(def_sampling_ms, def_sampling_ms);
+show_one(debug_intelli_plug, debug_intelli_plug);
+show_one(nr_fshift, nr_fshift);
+show_one(nr_run_hysteresis, nr_run_hysteresis);
+show_one(down_lock_duration, down_lock_dur);
+
+#define store_one(file_name, object)		\
+static ssize_t store_##file_name		\
+(struct kobject *kobj,				\
+ struct kobj_attribute *attr,			\
+ const char *buf, size_t count)			\
+{						\
+	unsigned int input;			\
+	int ret;				\
+	ret = sscanf(buf, "%u", &input);	\
+	if (ret != 1 || input > 100)		\
+		return -EINVAL;			\
+	if (input == object) {			\
+		return count;			\
+	}					\
+	object = input;				\
+	return count;				\
+}
+
+store_one(cpus_boosted, cpus_boosted);
+store_one(suspend_defer_time, suspend_defer_time);
+store_one(hotplug_suspend, hotplug_suspend);
+store_one(full_mode_profile, full_mode_profile);
+store_one(cpu_nr_run_threshold, cpu_nr_run_threshold);
+store_one(def_sampling_ms, def_sampling_ms);
+store_one(debug_intelli_plug, debug_intelli_plug);
+store_one(nr_fshift, nr_fshift);
+store_one(nr_run_hysteresis, nr_run_hysteresis);
+store_one(down_lock_duration, down_lock_dur);
+
+static ssize_t show_intelli_plug_active(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					char *buf)
+{
+	return sprintf(buf, "%d\n",
+			atomic_read(&intelli_plug_active));
+}
+
+static ssize_t store_intelli_plug_active(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 const char *buf, size_t count)
+{
+	int ret;
+	unsigned int input;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret < 0)
+		return ret;
+
+	if (input < 0)
+		input = 0;
+	else if (input > 0)
+		input = 1;
+
+	if (input == atomic_read(&intelli_plug_active))
+		return count;
+
+	intelli_plug_active_eval_fn(input);
+
+	return count;
+}
+
+static ssize_t show_boost_lock_duration(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					char *buf)
+{
+	return sprintf(buf, "%llu\n", div_u64(boost_lock_duration, 1000));
+}
+
+static ssize_t store_boost_lock_duration(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 const char *buf, size_t count)
+{
+	int ret;
+	u64 val;
+
+	ret = sscanf(buf, "%llu", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	boost_lock_duration = val * 1000;
+
+	return count;
+}
+
+static ssize_t store_min_cpus_online(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 1 || val > NR_CPUS)
+		return -EINVAL;
+
+	if (max_cpus_online < val)
+		max_cpus_online = val;
+
+	min_cpus_online = val;
+
+	return count;
+}
+
+static ssize_t store_max_cpus_online(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 1 || val > NR_CPUS)
+		return -EINVAL;
+
+	if (min_cpus_online > val)
+		min_cpus_online = val;
+
+	max_cpus_online = val;
+
+	return count;
+}
+
+#define KERNEL_ATTR_RW(_name) \
+static struct kobj_attribute _name##_attr = \
+	__ATTR(_name, 0644, show_##_name, store_##_name)
+
+KERNEL_ATTR_RW(intelli_plug_active);
+KERNEL_ATTR_RW(cpus_boosted);
+KERNEL_ATTR_RW(min_cpus_online);
+KERNEL_ATTR_RW(max_cpus_online);
+KERNEL_ATTR_RW(suspend_defer_time);
+KERNEL_ATTR_RW(hotplug_suspend);
+KERNEL_ATTR_RW(full_mode_profile);
+KERNEL_ATTR_RW(cpu_nr_run_threshold);
+KERNEL_ATTR_RW(boost_lock_duration);
+KERNEL_ATTR_RW(def_sampling_ms);
+KERNEL_ATTR_RW(debug_intelli_plug);
+KERNEL_ATTR_RW(nr_fshift);
+KERNEL_ATTR_RW(nr_run_hysteresis);
+KERNEL_ATTR_RW(down_lock_duration);
+
+static struct attribute *intelli_plug_attrs[] = {
+	&intelli_plug_active_attr.attr,
+	&cpus_boosted_attr.attr,
+	&min_cpus_online_attr.attr,
+	&max_cpus_online_attr.attr,
+	&suspend_defer_time_attr.attr,
+	&hotplug_suspend_attr.attr,
+	&full_mode_profile_attr.attr,
+	&cpu_nr_run_threshold_attr.attr,
+	&boost_lock_duration_attr.attr,
+	&def_sampling_ms_attr.attr,
+	&debug_intelli_plug_attr.attr,
+	&nr_fshift_attr.attr,
+	&nr_run_hysteresis_attr.attr,
+	&down_lock_duration_attr.attr,
+	NULL,
+};
+
+static struct attribute_group intelli_plug_attr_group = {
+	.attrs = intelli_plug_attrs,
+	.name = "intelli_plug",
+};
+
+static int __init intelli_plug_init(void)
+{
+	int rc;
+
+	rc = sysfs_create_group(kernel_kobj, &intelli_plug_attr_group);
+
+	pr_info("intelli_plug: version %d.%d\n",
+		 INTELLI_PLUG_MAJOR_VERSION,
+		 INTELLI_PLUG_MINOR_VERSION);
+
+	if (atomic_read(&intelli_plug_active) == 1)
+		intelli_plug_start();
+
+	return 0;
+}
+
+static void __exit intelli_plug_exit(void)
+{
+	if (atomic_read(&intelli_plug_active) == 1)
+		intelli_plug_stop();
+
+	sysfs_remove_group(kernel_kobj, &intelli_plug_attr_group);
+}
+
+late_initcall(intelli_plug_init);
+module_exit(intelli_plug_exit);
+
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>, \
+		Alucard24, Dorimanx, neobuddy89");
+MODULE_DESCRIPTION("'intell_plug' - An intelligent cpu hotplug driver for "
+	"Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPLv2");
diff --git a/arch/arm/mach-msm/msm_hotplug.c b/arch/arm/mach-msm/msm_hotplug.c
new file mode 100644
index 0000000..90ece66
--- /dev/null
+++ b/arch/arm/mach-msm/msm_hotplug.c
@@ -0,0 +1,1403 @@
+/*
+ * MSM Hotplug Driver
+ *
+ * Copyright (c) 2013-2014, Fluxi <linflux@arcor.de>
+ * Copyright (c) 2010-2014, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/cpu.h>
+#include <linux/init.h>
+#include <linux/workqueue.h>
+#include <linux/sched.h>
+#include <linux/platform_device.h>
+#include <linux/device.h>
+#include <linux/slab.h>
+#include <linux/cpufreq.h>
+#include <linux/mutex.h>
+#include <linux/input.h>
+#include <linux/math64.h>
+#include <linux/kernel_stat.h>
+#include <linux/tick.h>
+
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#else
+#include <linux/fb.h>
+#endif
+
+#define MSM_HOTPLUG			"msm_hotplug"
+#define HOTPLUG_ENABLED			1
+#define DEFAULT_UPDATE_RATE		HZ / 10
+#define START_DELAY			HZ * 10
+#define MIN_INPUT_INTERVAL		150 * 1000L
+#define DEFAULT_HISTORY_SIZE		10
+#define DEFAULT_DOWN_LOCK_DUR		1000
+#define DEFAULT_BOOST_LOCK_DUR		500 * 1000L
+#define DEFAULT_NR_CPUS_BOOSTED		2
+#define DEFAULT_MIN_CPUS_ONLINE		4
+#define DEFAULT_MAX_CPUS_ONLINE		NR_CPUS
+/* cur_avg_load can be > 200! */
+#define DEFAULT_FAST_LANE_LOAD		180
+#define DEFAULT_FAST_LANE_MIN_FREQ	1574400
+#define DEFAULT_SUSPEND_DEFER_TIME	10
+
+/*
+ * debug = 1 will print info with dprintk.
+ * debug = 2 will print suspend/resume only.
+ * debug = 3 will print suspend/resume and fast lane boost.
+ * debug = 4 will print suspend/resume and hotplug work delay for debug.
+ */
+static unsigned int debug = 2;
+module_param_named(debug_mask, debug, uint, 0644);
+
+/*
+ * suspend mode, if set = 1 hotplug will sleep,
+ * if set = 0, then hoplug will be active all the time.
+ */
+static unsigned int hotplug_suspend = 0;
+module_param_named(hotplug_suspend, hotplug_suspend, uint, 0644);
+
+#define dprintk(msg...)		\
+do {				\
+	if (debug)		\
+		pr_info(msg);	\
+} while (0)
+
+static struct cpu_hotplug {
+	unsigned int msm_enabled;
+	unsigned int suspended;
+	unsigned int suspend_defer_time;
+	unsigned int min_cpus_online_res;
+	unsigned int max_cpus_online_res;
+	unsigned int target_cpus;
+	unsigned int min_cpus_online;
+	unsigned int max_cpus_online;
+	unsigned int cpus_boosted;
+	unsigned int offline_load;
+	unsigned int down_lock_dur;
+	uint32_t fast_lane_min_freq;
+	u64 boost_lock_dur;
+	u64 last_input;
+	unsigned int fast_lane_load;
+	struct work_struct up_work;
+	struct work_struct down_work;
+	struct delayed_work suspend_work;
+	struct work_struct resume_work;
+	struct mutex msm_hotplug_mutex;
+#ifndef CONFIG_POWERSUSPEND
+	struct notifier_block notif;
+#endif
+} hotplug = {
+	.msm_enabled = HOTPLUG_ENABLED,
+	.min_cpus_online = DEFAULT_MIN_CPUS_ONLINE,
+	.max_cpus_online = DEFAULT_MAX_CPUS_ONLINE,
+	.suspended = 0,
+	.suspend_defer_time = DEFAULT_SUSPEND_DEFER_TIME,
+	.min_cpus_online_res = DEFAULT_MIN_CPUS_ONLINE,
+	.max_cpus_online_res = DEFAULT_MAX_CPUS_ONLINE,
+	.cpus_boosted = DEFAULT_NR_CPUS_BOOSTED,
+	.down_lock_dur = DEFAULT_DOWN_LOCK_DUR,
+	.boost_lock_dur = DEFAULT_BOOST_LOCK_DUR,
+	.fast_lane_load = DEFAULT_FAST_LANE_LOAD,
+	.fast_lane_min_freq = DEFAULT_FAST_LANE_MIN_FREQ
+};
+
+static struct workqueue_struct *hotplug_wq;
+static struct workqueue_struct *susp_wq;
+static struct delayed_work hotplug_work;
+
+static u64 last_boost_time;
+static unsigned int default_update_rates[] = { DEFAULT_UPDATE_RATE };
+
+static struct cpu_stats {
+	unsigned int *update_rates;
+	int nupdate_rates;
+	spinlock_t update_rates_lock;
+	unsigned int *load_hist;
+	unsigned int hist_size;
+	unsigned int hist_cnt;
+	unsigned int min_cpus;
+	unsigned int total_cpus;
+	unsigned int online_cpus;
+	unsigned int cur_avg_load;
+	struct mutex stats_mutex;
+} stats = {
+	.update_rates = default_update_rates,
+	.nupdate_rates = ARRAY_SIZE(default_update_rates),
+	.hist_size = DEFAULT_HISTORY_SIZE,
+	.min_cpus = 1,
+	.total_cpus = NR_CPUS
+};
+
+struct down_lock {
+	unsigned int locked;
+	struct delayed_work lock_rem;
+};
+
+static DEFINE_PER_CPU(struct down_lock, lock_info);
+
+struct cpu_load_data {
+	u64 prev_cpu_idle;
+	u64 prev_cpu_wall;
+	unsigned int avg_load_maxfreq;
+	unsigned int cur_load_maxfreq;
+	unsigned int samples;
+	unsigned int window_size;
+	cpumask_var_t related_cpus;
+};
+
+static DEFINE_PER_CPU(struct cpu_load_data, cpuload);
+
+static bool io_is_busy;
+
+static int update_average_load(unsigned int cpu)
+{
+	int ret;
+	unsigned int idle_time, wall_time;
+	unsigned int cur_load, load_max_freq;
+	u64 cur_wall_time, cur_idle_time;
+	struct cpu_load_data *pcpu = &per_cpu(cpuload, cpu);
+	struct cpufreq_policy policy;
+
+	ret = cpufreq_get_policy(&policy, cpu);
+	if (ret)
+		return -EINVAL;
+
+	cur_idle_time = get_cpu_idle_time(cpu, &cur_wall_time, io_is_busy);
+
+	wall_time = (unsigned int) (cur_wall_time - pcpu->prev_cpu_wall);
+	pcpu->prev_cpu_wall = cur_wall_time;
+
+	idle_time = (unsigned int) (cur_idle_time - pcpu->prev_cpu_idle);
+	pcpu->prev_cpu_idle = cur_idle_time;
+
+	if (unlikely(!wall_time || wall_time < idle_time))
+		return 0;
+
+	cur_load = 100 * (wall_time - idle_time) / wall_time;
+
+	/* Calculate the scaled load across cpu */
+	load_max_freq = (cur_load * policy.cur) / policy.max;
+
+	if (!pcpu->avg_load_maxfreq) {
+		/* This is the first sample in this window */
+		pcpu->avg_load_maxfreq = load_max_freq;
+		pcpu->window_size = wall_time;
+	} else {
+		/*
+		 * The is already a sample available in this window.
+		 * Compute weighted average with prev entry, so that
+		 * we get the precise weighted load.
+		 */
+		pcpu->avg_load_maxfreq =
+			((pcpu->avg_load_maxfreq * pcpu->window_size) +
+			(load_max_freq * wall_time)) /
+			(wall_time + pcpu->window_size);
+
+		pcpu->window_size += wall_time;
+	}
+
+	return 0;
+}
+
+static unsigned int load_at_max_freq(void)
+{
+	int cpu;
+	unsigned int total_load = 0, max_load = 0;
+	struct cpu_load_data *pcpu;
+
+	for_each_online_cpu(cpu) {
+		pcpu = &per_cpu(cpuload, cpu);
+		update_average_load(cpu);
+		total_load += pcpu->avg_load_maxfreq;
+		pcpu->cur_load_maxfreq = pcpu->avg_load_maxfreq;
+		max_load = max(max_load, pcpu->avg_load_maxfreq);
+		pcpu->avg_load_maxfreq = 0;
+	}
+
+	return total_load;
+}
+static void update_load_stats(void)
+{
+	unsigned int i, j;
+	unsigned int load = 0;
+
+	mutex_lock(&stats.stats_mutex);
+	stats.online_cpus = num_online_cpus();
+
+	if (stats.hist_size > 1) {
+		stats.load_hist[stats.hist_cnt] = load_at_max_freq();
+	} else {
+		stats.cur_avg_load = load_at_max_freq();
+		mutex_unlock(&stats.stats_mutex);
+		return;
+	}
+
+	for (i = 0, j = stats.hist_cnt; i < stats.hist_size; i++, j--) {
+		load += stats.load_hist[j];
+
+		if (j == 0)
+			j = stats.hist_size;
+	}
+
+	if (++stats.hist_cnt == stats.hist_size)
+		stats.hist_cnt = 0;
+
+	stats.cur_avg_load = load / stats.hist_size;
+	mutex_unlock(&stats.stats_mutex);
+}
+
+struct loads_tbl {
+	unsigned int up_threshold;
+	unsigned int down_threshold;
+};
+
+#define LOAD_SCALE(u, d)     \
+{                            \
+	.up_threshold = u,   \
+	.down_threshold = d, \
+}
+
+static struct loads_tbl loads[] = {
+	LOAD_SCALE(400, 0),
+	LOAD_SCALE(65, 0),
+	LOAD_SCALE(120, 50),
+	LOAD_SCALE(190, 100),
+	LOAD_SCALE(410, 170),
+	LOAD_SCALE(0, 0),
+};
+
+static void apply_down_lock(unsigned int cpu)
+{
+	struct down_lock *dl = &per_cpu(lock_info, cpu);
+
+	dl->locked = 1;
+	mod_delayed_work_on(0, hotplug_wq, &dl->lock_rem,
+			      msecs_to_jiffies(hotplug.down_lock_dur));
+}
+
+static void remove_down_lock(struct work_struct *work)
+{
+	struct down_lock *dl = container_of(work, struct down_lock,
+					    lock_rem.work);
+	dl->locked = 0;
+}
+
+static int check_down_lock(unsigned int cpu)
+{
+	struct down_lock *dl = &per_cpu(lock_info, cpu);
+
+	return dl->locked;
+}
+
+static int get_lowest_load_cpu(void)
+{
+	int cpu, lowest_cpu = 0;
+	unsigned int lowest_load = UINT_MAX;
+	unsigned int cpu_load[stats.total_cpus];
+	unsigned int proj_load;
+	struct cpu_load_data *pcpu;
+
+	for_each_online_cpu(cpu) {
+		if (cpu == 0)
+			continue;
+		pcpu = &per_cpu(cpuload, cpu);
+		cpu_load[cpu] = pcpu->cur_load_maxfreq;
+		if (cpu_load[cpu] < lowest_load) {
+			lowest_load = cpu_load[cpu];
+			lowest_cpu = cpu;
+		}
+	}
+
+	proj_load = stats.cur_avg_load - lowest_load;
+	if (proj_load > loads[stats.online_cpus - 1].up_threshold)
+		return -EPERM;
+
+	if (hotplug.offline_load && lowest_load >= hotplug.offline_load)
+		return -EPERM;
+
+	return lowest_cpu;
+}
+
+static void __ref cpu_up_work(struct work_struct *work)
+{
+	int cpu;
+	unsigned int target;
+
+	target = hotplug.target_cpus;
+
+	for_each_cpu_not(cpu, cpu_online_mask) {
+		if (target <= num_online_cpus())
+			break;
+		if (cpu == 0)
+			continue;
+		cpu_up(cpu);
+		apply_down_lock(cpu);
+	}
+}
+
+static void cpu_down_work(struct work_struct *work)
+{
+	int cpu, lowest_cpu;
+	unsigned int target;
+
+	target = hotplug.target_cpus;
+
+	for_each_online_cpu(cpu) {
+		if (cpu == 0)
+			continue;
+		lowest_cpu = get_lowest_load_cpu();
+		if (lowest_cpu > 0 && lowest_cpu <= stats.total_cpus) {
+			if (check_down_lock(lowest_cpu))
+				break;
+			cpu_down(lowest_cpu);
+		}
+		if (target >= num_online_cpus())
+			break;
+	}
+}
+
+static void online_cpu(unsigned int target)
+{
+	unsigned int online_cpus;
+
+	if (!hotplug.msm_enabled)
+		return;
+
+	online_cpus = num_online_cpus();
+
+	/*
+	 * Do not online more CPUs if max_cpus_online reached
+	 * and cancel online task if target already achieved.
+	 */
+	if (target <= online_cpus ||
+		online_cpus >= hotplug.max_cpus_online)
+		return;
+
+	hotplug.target_cpus = target;
+	queue_work_on(0, hotplug_wq, &hotplug.up_work);
+}
+
+static void offline_cpu(unsigned int target)
+{
+	unsigned int online_cpus;
+	u64 now;
+
+	if (!hotplug.msm_enabled)
+		return;
+
+	online_cpus = num_online_cpus();
+
+	/*
+	 * Do not offline more CPUs if min_cpus_online reached
+	 * and cancel offline task if target already achieved.
+	 */
+	if (target >= online_cpus ||
+		online_cpus <= hotplug.min_cpus_online)
+		return;
+
+	now = ktime_to_us(ktime_get());
+	if (online_cpus <= hotplug.cpus_boosted &&
+	    (now - hotplug.last_input < hotplug.boost_lock_dur))
+		return;
+
+	hotplug.target_cpus = target;
+	queue_work_on(0, hotplug_wq, &hotplug.down_work);
+}
+
+static unsigned int load_to_update_rate(unsigned int load)
+{
+	int i, ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&stats.update_rates_lock, flags);
+
+	for (i = 0; i < stats.nupdate_rates - 1 &&
+			load >= stats.update_rates[i+1]; i += 2)
+		;
+
+	ret = stats.update_rates[i];
+	spin_unlock_irqrestore(&stats.update_rates_lock, flags);
+	return ret;
+}
+
+static void reschedule_hotplug_work(void)
+{
+	unsigned int delay;
+
+	delay = load_to_update_rate(stats.cur_avg_load);
+	mod_delayed_work_on(0, hotplug_wq, &hotplug_work,
+			      msecs_to_jiffies(delay));
+	if (debug == 4)
+		pr_info("%s: reschedule_hotplug delay %u\n",
+				MSM_HOTPLUG, delay);
+}
+
+static void msm_hotplug_work(struct work_struct *work)
+{
+	unsigned int i, target = 0;
+
+	if (hotplug.suspended)
+		return;
+
+	update_load_stats();
+
+	if ((stats.cur_avg_load >= hotplug.fast_lane_load) &&
+			(cpufreq_quick_get(0) >= hotplug.fast_lane_min_freq)) {
+		/* Enter the fast lane */
+		online_cpu(hotplug.max_cpus_online);
+		if (debug == 3)
+			pr_info("%s: fast lane GO GO GO!\n", MSM_HOTPLUG);
+		goto reschedule;
+	}
+
+	/* If number of cpus locked, break out early */
+	if (hotplug.min_cpus_online == stats.total_cpus) {
+		if (stats.online_cpus != hotplug.min_cpus_online)
+			online_cpu(hotplug.min_cpus_online);
+		goto reschedule;
+	} else if (hotplug.max_cpus_online == stats.min_cpus) {
+		if (stats.online_cpus != hotplug.max_cpus_online)
+			offline_cpu(hotplug.max_cpus_online);
+		goto reschedule;
+	}
+
+	for (i = stats.min_cpus; loads[i].up_threshold; i++) {
+		if (stats.cur_avg_load <= loads[i].up_threshold
+		    && stats.cur_avg_load > loads[i].down_threshold) {
+			target = i;
+			break;
+		}
+	}
+
+	if (target > hotplug.max_cpus_online)
+		target = hotplug.max_cpus_online;
+	else if (target < hotplug.min_cpus_online)
+		target = hotplug.min_cpus_online;
+
+	if (stats.online_cpus != target) {
+		if (target > stats.online_cpus)
+			online_cpu(target);
+		else if (target < stats.online_cpus)
+			offline_cpu(target);
+	}
+
+reschedule:
+	if (debug == 1)
+		dprintk("%s: cur_avg_load: %3u online_cpus: %u target: %u\n",
+				MSM_HOTPLUG, stats.cur_avg_load,
+				stats.online_cpus, target);
+	reschedule_hotplug_work();
+}
+
+static void msm_hotplug_suspend(struct work_struct *work)
+{
+	if (!hotplug.suspended) {
+		mutex_lock(&hotplug.msm_hotplug_mutex);
+		hotplug.suspended = 1;
+		hotplug.min_cpus_online_res = hotplug.min_cpus_online;
+		hotplug.min_cpus_online = 1;
+		hotplug.max_cpus_online_res = hotplug.max_cpus_online;
+		hotplug.max_cpus_online = 1;
+		mutex_unlock(&hotplug.msm_hotplug_mutex);
+
+		/* Flush hotplug workqueue */
+		flush_workqueue(hotplug_wq);
+		cancel_delayed_work_sync(&hotplug_work);
+
+		if (debug >= 2)
+			dprintk("%s: suspended.\n", MSM_HOTPLUG);
+	}
+}
+
+static void __ref msm_hotplug_resume(struct work_struct *work)
+{
+	int cpu, required_reschedule = 0, required_wakeup = 0;
+
+	if (hotplug.suspended) {
+		mutex_lock(&hotplug.msm_hotplug_mutex);
+		hotplug.suspended = 0;
+		hotplug.min_cpus_online = hotplug.min_cpus_online_res;
+		hotplug.max_cpus_online = hotplug.max_cpus_online_res;
+		mutex_unlock(&hotplug.msm_hotplug_mutex);
+		required_wakeup = 1;
+		/* Initiate hotplug work */
+		required_reschedule = 1;
+		INIT_DELAYED_WORK(&hotplug_work, msm_hotplug_work);
+		if (debug >= 2)
+			dprintk("%s: resumed.\n", MSM_HOTPLUG);
+	}
+
+	if (required_wakeup) {
+		/* Fire up all CPUs */
+		for_each_cpu_not(cpu, cpu_online_mask) {
+			if (cpu == 0)
+				continue;
+			cpu_up(cpu);
+			apply_down_lock(cpu);
+		}
+		dprintk("%s: wakeup boosted.\n", MSM_HOTPLUG);
+	}
+
+	/* Resume hotplug workqueue if required */
+	if (required_reschedule)
+		reschedule_hotplug_work();
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __msm_hotplug_suspend(struct power_suspend *handler)
+#else
+static void __msm_hotplug_suspend(void)
+#endif
+{
+	if (!hotplug.msm_enabled || hotplug.suspended)
+		return;
+
+	if (!hotplug_suspend)
+		return;
+
+	INIT_DELAYED_WORK(&hotplug.suspend_work, msm_hotplug_suspend);
+	mod_delayed_work_on(0, susp_wq, &hotplug.suspend_work,
+			msecs_to_jiffies(hotplug.suspend_defer_time * 1000));
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void __ref __msm_hotplug_resume(struct power_suspend *handler)
+#else
+static void __ref __msm_hotplug_resume(void)
+#endif
+{
+	int cpu;
+
+	if (!hotplug.msm_enabled)
+		return;
+
+	if (!hotplug_suspend) {
+		/* Fire up all CPUs */
+		for_each_cpu_not(cpu, cpu_online_mask) {
+			if (cpu == 0)
+				continue;
+			cpu_up(cpu);
+			apply_down_lock(cpu);
+		}
+		dprintk("%s: wakeup boosted.\n", MSM_HOTPLUG);
+
+		return;
+	}
+
+	flush_workqueue(susp_wq);
+	cancel_delayed_work_sync(&hotplug.suspend_work);
+	queue_work_on(0, susp_wq, &hotplug.resume_work);
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static struct power_suspend msm_hotplug_power_suspend_driver = {
+	.suspend = __msm_hotplug_suspend,
+	.resume = __msm_hotplug_resume,
+};
+#else
+static int prev_fb = FB_BLANK_UNBLANK;
+
+static int fb_notifier_callback(struct notifier_block *self,
+				unsigned long event, void *data)
+{
+	struct fb_event *evdata = data;
+	int *blank;
+
+	if (evdata && evdata->data && event == FB_EVENT_BLANK) {
+		blank = evdata->data;
+		switch (*blank) {
+			case FB_BLANK_UNBLANK:
+				if (prev_fb == FB_BLANK_POWERDOWN) {
+					/* display on */
+					__msm_hotplug_resume();
+					prev_fb = FB_BLANK_UNBLANK;
+				}
+				break;
+			case FB_BLANK_POWERDOWN:
+				if (prev_fb == FB_BLANK_UNBLANK) {
+					/* display off */
+					__msm_hotplug_suspend();
+					prev_fb = FB_BLANK_POWERDOWN;
+				}
+				break;
+		}
+	}
+
+	return NOTIFY_OK;
+}
+#endif
+
+static void hotplug_input_event(struct input_handle *handle, unsigned int type,
+				unsigned int code, int value)
+{
+	u64 now;
+
+	if (hotplug.suspended)
+		return;
+
+	now = ktime_to_us(ktime_get());
+	hotplug.last_input = now;
+	if (now - last_boost_time < MIN_INPUT_INTERVAL)
+		return;
+
+	if (num_online_cpus() >= hotplug.cpus_boosted ||
+		hotplug.cpus_boosted <= hotplug.min_cpus_online)
+		return;
+
+	if (debug == 1)
+		dprintk("%s: online_cpus: %u boosted\n", MSM_HOTPLUG,
+				stats.online_cpus);
+
+	online_cpu(hotplug.cpus_boosted);
+	last_boost_time = ktime_to_us(ktime_get());
+}
+
+static int hotplug_input_connect(struct input_handler *handler,
+				 struct input_dev *dev,
+				 const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int err;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = handler->name;
+
+	err = input_register_handle(handle);
+	if (err)
+		goto err_register;
+
+	err = input_open_device(handle);
+	if (err)
+		goto err_open;
+
+	return 0;
+err_open:
+	input_unregister_handle(handle);
+err_register:
+	kfree(handle);
+	return err;
+}
+
+static void hotplug_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id hotplug_ids[] = {
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_MT_POSITION_X)] =
+			    BIT_MASK(ABS_MT_POSITION_X) |
+			    BIT_MASK(ABS_MT_POSITION_Y) },
+	}, /* multi-touch touchscreen */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_KEYBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.keybit = { [BIT_WORD(BTN_TOUCH)] = BIT_MASK(BTN_TOUCH) },
+		.absbit = { [BIT_WORD(ABS_X)] =
+			    BIT_MASK(ABS_X) | BIT_MASK(ABS_Y) },
+	}, /* touchpad */
+	{ },
+};
+
+static struct input_handler hotplug_input_handler = {
+	.event		= hotplug_input_event,
+	.connect	= hotplug_input_connect,
+	.disconnect	= hotplug_input_disconnect,
+	.name		= MSM_HOTPLUG,
+	.id_table	= hotplug_ids,
+};
+
+static int __ref msm_hotplug_start(void)
+{
+	int cpu, ret = 0;
+	struct down_lock *dl;
+
+	hotplug_wq =
+	    alloc_workqueue("msm_hotplug_wq", WQ_HIGHPRI | WQ_FREEZABLE, 0);
+	if (!hotplug_wq) {
+		pr_err("%s: Failed to allocate hotplug workqueue\n",
+		       MSM_HOTPLUG);
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+	susp_wq =
+		alloc_workqueue("susp_wq", WQ_FREEZABLE, 0);
+	if (!susp_wq) {
+		pr_err("%s: Failed to allocate suspend workqueue\n",
+				MSM_HOTPLUG);
+		ret = -ENOMEM;
+		goto err_out;
+	}
+
+#ifdef CONFIG_POWERSUSPEND
+	register_power_suspend(&msm_hotplug_power_suspend_driver);
+#else
+	hotplug.notif.notifier_call = fb_notifier_callback;
+	if (fb_register_client(&hotplug.notif)) {
+		pr_err("%s: Failed to register FB notifier callback\n",
+			MSM_HOTPLUG);
+		goto err_dev;
+	}
+#endif
+
+	ret = input_register_handler(&hotplug_input_handler);
+	if (ret) {
+		pr_err("%s: Failed to register input handler: %d\n",
+		       MSM_HOTPLUG, ret);
+		goto err_dev;
+	}
+
+	stats.load_hist = kmalloc(sizeof(stats.hist_size), GFP_KERNEL);
+	if (!stats.load_hist) {
+		pr_err("%s: Failed to allocate memory\n", MSM_HOTPLUG);
+		ret = -ENOMEM;
+		goto err_dev;
+	}
+
+	mutex_init(&stats.stats_mutex);
+	mutex_init(&hotplug.msm_hotplug_mutex);
+
+	INIT_DELAYED_WORK(&hotplug_work, msm_hotplug_work);
+	INIT_WORK(&hotplug.up_work, cpu_up_work);
+	INIT_WORK(&hotplug.down_work, cpu_down_work);
+	for_each_possible_cpu(cpu) {
+		dl = &per_cpu(lock_info, cpu);
+		INIT_DELAYED_WORK(&dl->lock_rem, remove_down_lock);
+	}
+	INIT_DELAYED_WORK(&hotplug.suspend_work, msm_hotplug_suspend);
+	INIT_WORK(&hotplug.resume_work, msm_hotplug_resume);
+
+	/* Put all sibling cores to sleep */
+	for_each_online_cpu(cpu) {
+		if (cpu == 0)
+			continue;
+		cpu_down(cpu);
+	}
+
+	mod_delayed_work_on(0, hotplug_wq, &hotplug_work,
+							START_DELAY);
+
+	return ret;
+err_dev:
+	destroy_workqueue(hotplug_wq);
+err_out:
+	hotplug.msm_enabled = 0;
+	return ret;
+}
+
+static void msm_hotplug_stop(void)
+{
+	int cpu;
+	struct down_lock *dl;
+
+	flush_workqueue(susp_wq);
+	cancel_work_sync(&hotplug.resume_work);
+	cancel_delayed_work_sync(&hotplug.suspend_work);
+	flush_workqueue(hotplug_wq);
+	for_each_possible_cpu(cpu) {
+		dl = &per_cpu(lock_info, cpu);
+		cancel_delayed_work_sync(&dl->lock_rem);
+	}
+	cancel_work_sync(&hotplug.down_work);
+	cancel_work_sync(&hotplug.up_work);
+	cancel_delayed_work_sync(&hotplug_work);
+
+	mutex_destroy(&hotplug.msm_hotplug_mutex);
+	mutex_destroy(&stats.stats_mutex);
+	kfree(stats.load_hist);
+
+#ifdef CONFIG_POWERSUSPEND
+	unregister_power_suspend(&msm_hotplug_power_suspend_driver);
+#else
+	fb_unregister_client(&hotplug.notif);
+	hotplug.notif.notifier_call = NULL;
+#endif
+	input_unregister_handler(&hotplug_input_handler);
+
+	destroy_workqueue(susp_wq);
+	destroy_workqueue(hotplug_wq);
+}
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%d", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_enable_hotplug(struct device *dev,
+				   struct device_attribute *msm_hotplug_attrs,
+				   char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.msm_enabled);
+}
+
+static ssize_t store_enable_hotplug(struct device *dev,
+				    struct device_attribute *msm_hotplug_attrs,
+				    const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 0 || val > 1)
+		return -EINVAL;
+
+	if (val == hotplug.msm_enabled)
+		return count;
+
+	hotplug.msm_enabled = val;
+
+	if (hotplug.msm_enabled)
+		ret = msm_hotplug_start();
+	else
+		msm_hotplug_stop();
+
+	return count;
+}
+
+static ssize_t show_down_lock_duration(struct device *dev,
+				       struct device_attribute
+				       *msm_hotplug_attrs, char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.down_lock_dur);
+}
+
+static ssize_t store_down_lock_duration(struct device *dev,
+					struct device_attribute
+					*msm_hotplug_attrs, const char *buf,
+					size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.down_lock_dur = val;
+
+	return count;
+}
+
+static ssize_t show_boost_lock_duration(struct device *dev,
+				        struct device_attribute
+				        *msm_hotplug_attrs, char *buf)
+{
+	return sprintf(buf, "%llu\n", div_u64(hotplug.boost_lock_dur, 1000));
+}
+
+static ssize_t store_boost_lock_duration(struct device *dev,
+					 struct device_attribute
+					 *msm_hotplug_attrs, const char *buf,
+					 size_t count)
+{
+	int ret;
+	u64 val;
+
+	ret = sscanf(buf, "%llu", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.boost_lock_dur = val * 1000;
+
+	return count;
+}
+
+static ssize_t show_update_rates(struct device *dev,
+				struct device_attribute *msm_hotplug_attrs,
+				char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&stats.update_rates_lock, flags);
+
+	for (i = 0; i < stats.nupdate_rates; i++)
+		ret += sprintf(buf + ret, "%u%s", stats.update_rates[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&stats.update_rates_lock, flags);
+	return ret;
+}
+
+static ssize_t store_update_rates(struct device *dev,
+				 struct device_attribute *msm_hotplug_attrs,
+				 const char *buf, size_t count)
+{
+	int ntokens;
+	unsigned int *new_update_rates = NULL;
+	unsigned long flags;
+
+	new_update_rates = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_update_rates))
+		return PTR_RET(new_update_rates);
+
+	spin_lock_irqsave(&stats.update_rates_lock, flags);
+	if (stats.update_rates != default_update_rates)
+		kfree(stats.update_rates);
+	stats.update_rates = new_update_rates;
+	stats.nupdate_rates = ntokens;
+	spin_unlock_irqrestore(&stats.update_rates_lock, flags);
+	return count;
+}
+
+static ssize_t show_load_levels(struct device *dev,
+				struct device_attribute *msm_hotplug_attrs,
+				char *buf)
+{
+	int i, len = 0;
+
+	if (!buf)
+		return -EINVAL;
+
+	for (i = 0; loads[i].up_threshold; i++) {
+		len += sprintf(buf + len, "%u ", i);
+		len += sprintf(buf + len, "%u ", loads[i].up_threshold);
+		len += sprintf(buf + len, "%u\n", loads[i].down_threshold);
+	}
+
+	return len;
+}
+
+static ssize_t store_load_levels(struct device *dev,
+				 struct device_attribute *msm_hotplug_attrs,
+				 const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val[3];
+
+	ret = sscanf(buf, "%u %u %u", &val[0], &val[1], &val[2]);
+	if (ret != ARRAY_SIZE(val) || val[2] > val[1])
+		return -EINVAL;
+
+	loads[val[0]].up_threshold = val[1];
+	loads[val[0]].down_threshold = val[2];
+
+	return count;
+}
+
+static ssize_t show_history_size(struct device *dev,
+				 struct device_attribute *msm_hotplug_attrs,
+				 char *buf)
+{
+	return sprintf(buf, "%u\n", stats.hist_size);
+}
+
+static ssize_t store_history_size(struct device *dev,
+				  struct device_attribute *msm_hotplug_attrs,
+				  const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 1 || val > 20)
+		return -EINVAL;
+
+	if (hotplug.msm_enabled) {
+		flush_workqueue(hotplug_wq);
+		cancel_delayed_work_sync(&hotplug_work);
+		memset(stats.load_hist, 0, sizeof(stats.load_hist));
+	}
+
+	stats.hist_size = val;
+
+	if (hotplug.msm_enabled)
+		reschedule_hotplug_work();
+
+	return count;
+}
+
+static ssize_t show_min_cpus_online(struct device *dev,
+				    struct device_attribute *msm_hotplug_attrs,
+				    char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.min_cpus_online);
+}
+
+static ssize_t store_min_cpus_online(struct device *dev,
+				struct device_attribute *msm_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 1 || val > stats.total_cpus)
+		return -EINVAL;
+
+	if (hotplug.max_cpus_online < val)
+		hotplug.max_cpus_online = val;
+
+	hotplug.min_cpus_online = val;
+
+	return count;
+}
+
+static ssize_t show_max_cpus_online(struct device *dev,
+				    struct device_attribute *msm_hotplug_attrs,
+				    char *buf)
+{
+	return sprintf(buf, "%u\n",hotplug.max_cpus_online);
+}
+
+static ssize_t store_max_cpus_online(struct device *dev,
+				struct device_attribute *msm_hotplug_attrs,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 1 || val > stats.total_cpus)
+		return -EINVAL;
+
+	if (hotplug.min_cpus_online > val)
+		hotplug.min_cpus_online = val;
+
+	hotplug.max_cpus_online = val;
+
+	return count;
+}
+
+static ssize_t store_suspend_defer_time(struct device *dev,
+				    struct device_attribute *msm_hotplug_attrs,
+				    const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.suspend_defer_time = val;
+
+	return count;
+}
+
+static ssize_t show_suspend_defer_time(struct device *dev,
+				 struct device_attribute *msm_hotplug_attrs,
+				 char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.suspend_defer_time);
+}
+
+static ssize_t show_cpus_boosted(struct device *dev,
+				 struct device_attribute *msm_hotplug_attrs,
+				 char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.cpus_boosted);
+}
+
+static ssize_t store_cpus_boosted(struct device *dev,
+				  struct device_attribute *msm_hotplug_attrs,
+				  const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 1 || val > stats.total_cpus)
+		return -EINVAL;
+
+	hotplug.cpus_boosted = val;
+
+	return count;
+}
+
+static ssize_t show_offline_load(struct device *dev,
+				 struct device_attribute *msm_hotplug_attrs,
+				 char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.offline_load);
+}
+
+static ssize_t store_offline_load(struct device *dev,
+				  struct device_attribute *msm_hotplug_attrs,
+				  const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.offline_load = val;
+
+	return count;
+}
+
+static ssize_t show_fast_lane_load(struct device *dev,
+				   struct device_attribute *msm_hotplug_attrs,
+				   char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.fast_lane_load);
+}
+
+static ssize_t store_fast_lane_load(struct device *dev,
+				    struct device_attribute *msm_hotplug_attrs,
+				    const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.fast_lane_load = val;
+
+	return count;
+}
+
+static ssize_t show_fast_lane_min_freq(struct device *dev,
+				   struct device_attribute *msm_hotplug_attrs,
+				   char *buf)
+{
+	return sprintf(buf, "%u\n", hotplug.fast_lane_min_freq);
+}
+
+static ssize_t store_fast_lane_min_freq(struct device *dev,
+				    struct device_attribute *msm_hotplug_attrs,
+				    const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	hotplug.fast_lane_min_freq = val;
+
+	return count;
+}
+
+static ssize_t show_io_is_busy(struct device *dev,
+				   struct device_attribute *msm_hotplug_attrs,
+				   char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct device *dev,
+				    struct device_attribute *msm_hotplug_attrs,
+				    const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+
+	ret = sscanf(buf, "%u", &val);
+	if (ret != 1 || val < 0 || val > 1)
+		return -EINVAL;
+
+	io_is_busy = val ? true : false;
+
+	return count;
+}
+
+static ssize_t show_current_load(struct device *dev,
+				 struct device_attribute *msm_hotplug_attrs,
+				 char *buf)
+{
+	return sprintf(buf, "%u\n", stats.cur_avg_load);
+}
+
+static DEVICE_ATTR(msm_enabled, 644, show_enable_hotplug,
+		   store_enable_hotplug);
+static DEVICE_ATTR(down_lock_duration, 644, show_down_lock_duration,
+		   store_down_lock_duration);
+static DEVICE_ATTR(boost_lock_duration, 644, show_boost_lock_duration,
+		   store_boost_lock_duration);
+static DEVICE_ATTR(update_rates, 644, show_update_rates, store_update_rates);
+static DEVICE_ATTR(load_levels, 644, show_load_levels, store_load_levels);
+static DEVICE_ATTR(history_size, 644, show_history_size, store_history_size);
+static DEVICE_ATTR(min_cpus_online, 644, show_min_cpus_online,
+		   store_min_cpus_online);
+static DEVICE_ATTR(max_cpus_online, 644, show_max_cpus_online,
+		   store_max_cpus_online);
+static DEVICE_ATTR(suspend_defer_time, 644, show_suspend_defer_time,
+		   store_suspend_defer_time);
+static DEVICE_ATTR(cpus_boosted, 644, show_cpus_boosted, store_cpus_boosted);
+static DEVICE_ATTR(offline_load, 644, show_offline_load, store_offline_load);
+static DEVICE_ATTR(fast_lane_load, 644, show_fast_lane_load,
+		   store_fast_lane_load);
+static DEVICE_ATTR(fast_lane_min_freq, 644, show_fast_lane_min_freq,
+		   store_fast_lane_min_freq);
+static DEVICE_ATTR(io_is_busy, 644, show_io_is_busy, store_io_is_busy);
+static DEVICE_ATTR(current_load, 444, show_current_load, NULL);
+
+static struct attribute *msm_hotplug_attrs[] = {
+	&dev_attr_msm_enabled.attr,
+	&dev_attr_down_lock_duration.attr,
+	&dev_attr_boost_lock_duration.attr,
+	&dev_attr_update_rates.attr,
+	&dev_attr_load_levels.attr,
+	&dev_attr_history_size.attr,
+	&dev_attr_min_cpus_online.attr,
+	&dev_attr_max_cpus_online.attr,
+	&dev_attr_suspend_defer_time.attr,
+	&dev_attr_cpus_boosted.attr,
+	&dev_attr_offline_load.attr,
+	&dev_attr_io_is_busy.attr,
+	&dev_attr_fast_lane_load.attr,
+	&dev_attr_fast_lane_min_freq.attr,
+	&dev_attr_current_load.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = msm_hotplug_attrs,
+};
+
+/************************** sysfs end ************************/
+
+static int msm_hotplug_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct kobject *module_kobj;
+
+	module_kobj = kset_find_obj(module_kset, MSM_HOTPLUG);
+	if (!module_kobj) {
+		pr_err("%s: Cannot find kobject for module\n", MSM_HOTPLUG);
+		goto err_dev;
+	}
+
+	ret = sysfs_create_group(module_kobj, &attr_group);
+	if (ret) {
+		pr_err("%s: Failed to create sysfs: %d\n", MSM_HOTPLUG, ret);
+		goto err_dev;
+	}
+
+	if (hotplug.msm_enabled) {
+		ret = msm_hotplug_start();
+		if (ret != 0)
+			goto err_dev;
+	}
+
+	return ret;
+err_dev:
+	module_kobj = NULL;
+	return ret;
+}
+
+static struct platform_device msm_hotplug_device = {
+	.name = MSM_HOTPLUG,
+	.id = -1,
+};
+
+static int msm_hotplug_remove(struct platform_device *pdev)
+{
+	if (hotplug.msm_enabled)
+		msm_hotplug_stop();
+
+	return 0;
+}
+
+static struct platform_driver msm_hotplug_driver = {
+	.probe = msm_hotplug_probe,
+	.remove = msm_hotplug_remove,
+	.driver = {
+		.name = MSM_HOTPLUG,
+		.owner = THIS_MODULE,
+	},
+};
+
+static int __init msm_hotplug_init(void)
+{
+	int ret;
+
+	ret = platform_driver_register(&msm_hotplug_driver);
+	if (ret) {
+		pr_err("%s: Driver register failed: %d\n", MSM_HOTPLUG, ret);
+		return ret;
+	}
+
+	ret = platform_device_register(&msm_hotplug_device);
+	if (ret) {
+		pr_err("%s: Device register failed: %d\n", MSM_HOTPLUG, ret);
+		return ret;
+	}
+
+	pr_info("%s: Device init\n", MSM_HOTPLUG);
+
+	return ret;
+}
+
+static void __exit msm_hotplug_exit(void)
+{
+	platform_device_unregister(&msm_hotplug_device);
+	platform_driver_unregister(&msm_hotplug_driver);
+}
+
+late_initcall(msm_hotplug_init);
+module_exit(msm_hotplug_exit);
+
+MODULE_AUTHOR("Fluxi <linflux@arcor.de>, \
+				Pranav Vashi <neobuddy89@gmail.com>");
+MODULE_DESCRIPTION("MSM Hotplug Driver");
+MODULE_LICENSE("GPLv2");
diff --git a/include/linux/fastchg.h b/include/linux/fastchg.h
new file mode 100644
index 0000000..6f3b65f
--- /dev/null
+++ b/include/linux/fastchg.h
@@ -0,0 +1,41 @@
+/*
+ * based on work from:
+ *	Chad Froebel <chadfroebel@gmail.com> &
+ *	Jean-Pierre Rasquin <yank555.lu@gmail.com>
+ * for backwards compatibility
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _LINUX_FASTCHG_H
+#define _LINUX_FASTCHG_H
+
+extern int force_fast_charge;
+extern int force_fast_charge_temp;
+extern int fast_charge_level;
+extern int usb_power_curr_now;
+extern int force_fast_charge_on_off;
+
+#define FAST_CHARGE_DISABLED		0	/* default */
+#define FAST_CHARGE_FORCE_AC		1
+#define FAST_CHARGE_FORCE_CUSTOM_MA	2
+
+#define FAST_CHARGE_300		300
+#define FAST_CHARGE_500		500
+#define FAST_CHARGE_900		900
+#define FAST_CHARGE_1200	1200
+#define FAST_CHARGE_1600	1600
+#define FAST_CHARGE_1800	1800
+#define FAST_CHARGE_2000	2000
+
+#define FAST_CHARGE_LEVELS	"300 500 900 1200 1600 1800 2000"
+
+#endif
